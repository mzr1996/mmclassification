# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, OpenMMLab
# This file is distributed under the same license as the MMClassification
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MMClassification \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-12-14 17:43+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../api.rst:2
msgid "mmcls.apis"
msgstr ""

#: mmcls.apis.inference.inference_model:1 of
msgid "Inference image(s) with the classifier."
msgstr "使用分类器推理图像"

#: mmcls.apis.inference.inference_model mmcls.apis.inference.init_model
#: mmcls.apis.inference.show_result_pyplot mmcls.apis.test.multi_gpu_test
#: mmcls.apis.train.init_random_seed mmcls.apis.train.set_random_seed
#: mmcls.core.evaluation.eval_hooks.DistEvalHook
#: mmcls.core.evaluation.eval_hooks.EvalHook
#: mmcls.core.evaluation.eval_metrics.calculate_confusion_matrix
#: mmcls.core.evaluation.eval_metrics.f1_score
#: mmcls.core.evaluation.eval_metrics.precision
#: mmcls.core.evaluation.eval_metrics.precision_recall_f1
#: mmcls.core.evaluation.eval_metrics.recall
#: mmcls.core.evaluation.eval_metrics.support
#: mmcls.core.evaluation.mean_ap.average_precision
#: mmcls.core.evaluation.mean_ap.mAP
#: mmcls.core.evaluation.multilabel_eval_metrics.average_performance
#: mmcls.datasets.base_dataset.BaseDataset
#: mmcls.datasets.base_dataset.BaseDataset.evaluate
#: mmcls.datasets.base_dataset.BaseDataset.get_cat_ids
#: mmcls.datasets.base_dataset.BaseDataset.get_classes
#: mmcls.datasets.builder.build_dataloader
#: mmcls.datasets.dataset_wrappers.ClassBalancedDataset
#: mmcls.datasets.dataset_wrappers.ConcatDataset
#: mmcls.datasets.dataset_wrappers.RepeatDataset
#: mmcls.datasets.imagenet21k.ImageNet21k
#: mmcls.datasets.imagenet21k.ImageNet21k.get_cat_ids
#: mmcls.datasets.multi_label.MultiLabelDataset.evaluate
#: mmcls.datasets.multi_label.MultiLabelDataset.get_cat_ids
#: mmcls.datasets.pipelines.auto_augment.AutoAugment
#: mmcls.datasets.pipelines.auto_augment.AutoContrast
#: mmcls.datasets.pipelines.auto_augment.Brightness
#: mmcls.datasets.pipelines.auto_augment.ColorTransform
#: mmcls.datasets.pipelines.auto_augment.Contrast
#: mmcls.datasets.pipelines.auto_augment.Cutout
#: mmcls.datasets.pipelines.auto_augment.Equalize
#: mmcls.datasets.pipelines.auto_augment.Invert
#: mmcls.datasets.pipelines.auto_augment.Posterize
#: mmcls.datasets.pipelines.auto_augment.RandAugment
#: mmcls.datasets.pipelines.auto_augment.Rotate
#: mmcls.datasets.pipelines.auto_augment.Sharpness
#: mmcls.datasets.pipelines.auto_augment.Shear
#: mmcls.datasets.pipelines.auto_augment.Solarize
#: mmcls.datasets.pipelines.auto_augment.SolarizeAdd
#: mmcls.datasets.pipelines.auto_augment.Translate
#: mmcls.datasets.pipelines.compose.Compose
#: mmcls.datasets.pipelines.formatting.Collect
#: mmcls.datasets.pipelines.loading.LoadImageFromFile
#: mmcls.datasets.pipelines.transforms.CenterCrop
#: mmcls.datasets.pipelines.transforms.ColorJitter
#: mmcls.datasets.pipelines.transforms.Lighting
#: mmcls.datasets.pipelines.transforms.Normalize
#: mmcls.datasets.pipelines.transforms.Pad
#: mmcls.datasets.pipelines.transforms.RandomCrop
#: mmcls.datasets.pipelines.transforms.RandomCrop.get_params
#: mmcls.datasets.pipelines.transforms.RandomErasing
#: mmcls.datasets.pipelines.transforms.RandomFlip
#: mmcls.datasets.pipelines.transforms.RandomGrayscale
#: mmcls.datasets.pipelines.transforms.RandomResizedCrop
#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params
#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params_efficientnet_style
#: mmcls.datasets.pipelines.transforms.Resize
#: mmcls.models.backbones.alexnet.AlexNet
#: mmcls.models.backbones.alexnet.AlexNet.forward
#: mmcls.models.backbones.conformer.Conformer
#: mmcls.models.backbones.conformer.Conformer.forward
#: mmcls.models.backbones.lenet.LeNet5
#: mmcls.models.backbones.lenet.LeNet5.forward
#: mmcls.models.backbones.mlp_mixer.MlpMixer
#: mmcls.models.backbones.mlp_mixer.MlpMixer.forward
#: mmcls.models.backbones.mobilenet_v2.MobileNetV2
#: mmcls.models.backbones.mobilenet_v2.MobileNetV2.forward
#: mmcls.models.backbones.mobilenet_v2.MobileNetV2.make_layer
#: mmcls.models.backbones.mobilenet_v2.MobileNetV2.train
#: mmcls.models.backbones.mobilenet_v3.MobileNetV3
#: mmcls.models.backbones.mobilenet_v3.MobileNetV3.forward
#: mmcls.models.backbones.mobilenet_v3.MobileNetV3.train
#: mmcls.models.backbones.regnet.RegNet
#: mmcls.models.backbones.regnet.RegNet.adjust_width_group
#: mmcls.models.backbones.regnet.RegNet.forward
#: mmcls.models.backbones.regnet.RegNet.generate_regnet
#: mmcls.models.backbones.regnet.RegNet.get_stages_from_blocks
#: mmcls.models.backbones.regnet.RegNet.quantize_float
#: mmcls.models.backbones.repvgg.RepVGG
#: mmcls.models.backbones.repvgg.RepVGG.forward
#: mmcls.models.backbones.repvgg.RepVGG.train
#: mmcls.models.backbones.res2net.Res2Net
#: mmcls.models.backbones.resnest.ResNeSt mmcls.models.backbones.resnet.ResNet
#: mmcls.models.backbones.resnet.ResNet.forward
#: mmcls.models.backbones.resnet.ResNet.train
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR.forward
#: mmcls.models.backbones.resnext.ResNeXt
#: mmcls.models.backbones.seresnet.SEResNet
#: mmcls.models.backbones.seresnext.SEResNeXt
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1.forward
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1.make_layer
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1.train
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2.forward
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2.train
#: mmcls.models.backbones.swin_transformer.SwinTransformer
#: mmcls.models.backbones.swin_transformer.SwinTransformer.forward
#: mmcls.models.backbones.swin_transformer.SwinTransformer.train
#: mmcls.models.backbones.t2t_vit.T2T_ViT
#: mmcls.models.backbones.t2t_vit.T2T_ViT.forward
#: mmcls.models.backbones.timm_backbone.TIMMBackbone
#: mmcls.models.backbones.timm_backbone.TIMMBackbone.forward
#: mmcls.models.backbones.tnt.TNT mmcls.models.backbones.tnt.TNT.forward
#: mmcls.models.backbones.vgg.VGG mmcls.models.backbones.vgg.VGG.forward
#: mmcls.models.backbones.vgg.VGG.train
#: mmcls.models.backbones.vision_transformer.VisionTransformer
#: mmcls.models.backbones.vision_transformer.VisionTransformer.forward
#: mmcls.models.backbones.vision_transformer.VisionTransformer.resize_pos_embed
#: mmcls.models.classifiers.base.BaseClassifier.forward_test
#: mmcls.models.classifiers.base.BaseClassifier.forward_train
#: mmcls.models.classifiers.base.BaseClassifier.show_result
#: mmcls.models.classifiers.base.BaseClassifier.train_step
#: mmcls.models.classifiers.base.BaseClassifier.val_step
#: mmcls.models.classifiers.image.ImageClassifier.forward_train
#: mmcls.models.heads.cls_head.ClsHead
#: mmcls.models.heads.conformer_head.ConformerHead
#: mmcls.models.heads.linear_head.LinearClsHead
#: mmcls.models.heads.multi_label_head.MultiLabelClsHead
#: mmcls.models.heads.multi_label_linear_head.MultiLabelLinearClsHead
#: mmcls.models.heads.stacked_head.StackedLinearClsHead
#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead
#: mmcls.models.losses.accuracy.Accuracy.forward
#: mmcls.models.losses.accuracy.accuracy
#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss
#: mmcls.models.losses.asymmetric_loss.asymmetric_loss
#: mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss
#: mmcls.models.losses.cross_entropy_loss.binary_cross_entropy
#: mmcls.models.losses.cross_entropy_loss.cross_entropy
#: mmcls.models.losses.focal_loss.FocalLoss
#: mmcls.models.losses.focal_loss.FocalLoss.forward
#: mmcls.models.losses.focal_loss.sigmoid_focal_loss
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward
#: mmcls.models.losses.seesaw_loss.SeesawLoss
#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward
#: mmcls.models.losses.utils.convert_to_one_hot
#: mmcls.models.losses.utils.reduce_loss
#: mmcls.models.losses.utils.weight_reduce_loss
#: mmcls.models.necks.gap.GlobalAveragePooling
#: mmcls.models.utils.attention.MultiheadAttention
#: mmcls.models.utils.attention.ShiftWindowMSA
#: mmcls.models.utils.augment.augments.Augments
#: mmcls.models.utils.channel_shuffle.channel_shuffle
#: mmcls.models.utils.embed.HybridEmbed mmcls.models.utils.embed.PatchEmbed
#: mmcls.models.utils.embed.PatchMerging
#: mmcls.models.utils.inverted_residual.InvertedResidual
#: mmcls.models.utils.make_divisible.make_divisible
#: mmcls.models.utils.se_layer.SELayer mmcls.utils.logger.load_json_logs of
msgid "参数"
msgstr ""

#: mmcls.apis.inference.inference_model:3
#: mmcls.apis.inference.show_result_pyplot:3 of
msgid "The loaded classifier."
msgstr ""

#: mmcls.apis.inference.inference_model:5 of
msgid "The image filename or loaded image."
msgstr ""

#: mmcls.apis.inference.inference_model mmcls.apis.inference.init_model
#: mmcls.apis.test.multi_gpu_test mmcls.apis.train.init_random_seed
#: mmcls.core.evaluation.eval_metrics.calculate_confusion_matrix
#: mmcls.core.evaluation.eval_metrics.f1_score
#: mmcls.core.evaluation.eval_metrics.precision
#: mmcls.core.evaluation.eval_metrics.precision_recall_f1
#: mmcls.core.evaluation.eval_metrics.recall
#: mmcls.core.evaluation.eval_metrics.support
#: mmcls.core.evaluation.mean_ap.average_precision
#: mmcls.core.evaluation.mean_ap.mAP
#: mmcls.core.evaluation.multilabel_eval_metrics.average_performance
#: mmcls.datasets.BaseDataset.class_to_idx
#: mmcls.datasets.base_dataset.BaseDataset.evaluate
#: mmcls.datasets.base_dataset.BaseDataset.get_cat_ids
#: mmcls.datasets.base_dataset.BaseDataset.get_classes
#: mmcls.datasets.base_dataset.BaseDataset.get_gt_labels
#: mmcls.datasets.builder.build_dataloader
#: mmcls.datasets.imagenet21k.ImageNet21k.get_cat_ids
#: mmcls.datasets.multi_label.MultiLabelDataset.evaluate
#: mmcls.datasets.multi_label.MultiLabelDataset.get_cat_ids
#: mmcls.datasets.pipelines.formatting.Collect
#: mmcls.datasets.pipelines.transforms.RandomCrop.get_params
#: mmcls.datasets.pipelines.transforms.RandomGrayscale
#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params
#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params_efficientnet_style
#: mmcls.datasets.voc.VOC.load_annotations
#: mmcls.models.backbones.regnet.RegNet.adjust_width_group
#: mmcls.models.backbones.regnet.RegNet.generate_regnet
#: mmcls.models.backbones.regnet.RegNet.get_stages_from_blocks
#: mmcls.models.backbones.regnet.RegNet.quantize_float
#: mmcls.models.backbones.vision_transformer.VisionTransformer.resize_pos_embed
#: mmcls.models.classifiers.base.BaseClassifier.show_result
#: mmcls.models.classifiers.base.BaseClassifier.train_step
#: mmcls.models.classifiers.base.BaseClassifier.val_step
#: mmcls.models.classifiers.image.ImageClassifier.forward_train
#: mmcls.models.losses.accuracy.Accuracy.forward
#: mmcls.models.losses.accuracy.accuracy
#: mmcls.models.losses.asymmetric_loss.asymmetric_loss
#: mmcls.models.losses.cross_entropy_loss.binary_cross_entropy
#: mmcls.models.losses.cross_entropy_loss.cross_entropy
#: mmcls.models.losses.focal_loss.FocalLoss.forward
#: mmcls.models.losses.focal_loss.sigmoid_focal_loss
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward
#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward
#: mmcls.models.losses.utils.convert_to_one_hot
#: mmcls.models.losses.utils.reduce_loss
#: mmcls.models.losses.utils.weight_reduce_loss
#: mmcls.models.utils.channel_shuffle.channel_shuffle
#: mmcls.models.utils.inverted_residual.InvertedResidual
#: mmcls.models.utils.make_divisible.make_divisible
#: mmcls.utils.logger.load_json_logs of
msgid "返回"
msgstr ""

#: mmcls.apis.inference.inference_model:8 of
msgid ""
"The classification results that contains     `class_name`, `pred_label` "
"and `pred_score`."
msgstr ""

#: mmcls.apis.inference.inference_model:10 of
msgid "The classification results that contains"
msgstr ""

#: mmcls.apis.inference.inference_model:11 of
msgid "`class_name`, `pred_label` and `pred_score`."
msgstr ""

#: mmcls.apis.inference.inference_model mmcls.apis.inference.init_model
#: mmcls.apis.test.multi_gpu_test mmcls.apis.train.init_random_seed
#: mmcls.core.evaluation.eval_metrics.calculate_confusion_matrix
#: mmcls.core.evaluation.eval_metrics.f1_score
#: mmcls.core.evaluation.eval_metrics.precision
#: mmcls.core.evaluation.eval_metrics.precision_recall_f1
#: mmcls.core.evaluation.eval_metrics.recall
#: mmcls.core.evaluation.eval_metrics.support
#: mmcls.core.evaluation.mean_ap.average_precision
#: mmcls.core.evaluation.mean_ap.mAP
#: mmcls.core.evaluation.multilabel_eval_metrics.average_performance
#: mmcls.datasets.BaseDataset.class_to_idx
#: mmcls.datasets.base_dataset.BaseDataset.evaluate
#: mmcls.datasets.base_dataset.BaseDataset.get_cat_ids
#: mmcls.datasets.base_dataset.BaseDataset.get_classes
#: mmcls.datasets.base_dataset.BaseDataset.get_gt_labels
#: mmcls.datasets.builder.build_dataloader
#: mmcls.datasets.imagenet21k.ImageNet21k.get_cat_ids
#: mmcls.datasets.multi_label.MultiLabelDataset.evaluate
#: mmcls.datasets.multi_label.MultiLabelDataset.get_cat_ids
#: mmcls.datasets.pipelines.formatting.Collect
#: mmcls.datasets.pipelines.transforms.RandomCrop.get_params
#: mmcls.datasets.pipelines.transforms.RandomGrayscale
#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params
#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params_efficientnet_style
#: mmcls.datasets.voc.VOC.load_annotations
#: mmcls.models.backbones.regnet.RegNet.adjust_width_group
#: mmcls.models.backbones.regnet.RegNet.generate_regnet
#: mmcls.models.backbones.regnet.RegNet.get_stages_from_blocks
#: mmcls.models.backbones.regnet.RegNet.quantize_float
#: mmcls.models.backbones.vision_transformer.VisionTransformer.resize_pos_embed
#: mmcls.models.classifiers.base.BaseClassifier.show_result
#: mmcls.models.classifiers.base.BaseClassifier.train_step
#: mmcls.models.classifiers.base.BaseClassifier.val_step
#: mmcls.models.classifiers.image.ImageClassifier.forward_train
#: mmcls.models.losses.accuracy.Accuracy.forward
#: mmcls.models.losses.accuracy.accuracy
#: mmcls.models.losses.asymmetric_loss.asymmetric_loss
#: mmcls.models.losses.cross_entropy_loss.binary_cross_entropy
#: mmcls.models.losses.cross_entropy_loss.cross_entropy
#: mmcls.models.losses.focal_loss.FocalLoss.forward
#: mmcls.models.losses.focal_loss.sigmoid_focal_loss
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward
#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward
#: mmcls.models.losses.utils.convert_to_one_hot
#: mmcls.models.losses.utils.reduce_loss
#: mmcls.models.losses.utils.weight_reduce_loss
#: mmcls.models.utils.channel_shuffle.channel_shuffle
#: mmcls.models.utils.inverted_residual.InvertedResidual
#: mmcls.models.utils.make_divisible.make_divisible
#: mmcls.utils.logger.load_json_logs of
msgid "返回类型"
msgstr ""

#: mmcls.apis.inference.init_model:1 of
msgid "Initialize a classifier from config file."
msgstr ""

#: mmcls.apis.inference.init_model:3 of
msgid "Config file path or the config object."
msgstr ""

#: mmcls.apis.inference.init_model:6 of
msgid "Checkpoint path. If left as None, the model will not load any weights."
msgstr ""

#: mmcls.apis.inference.init_model:9 of
msgid "Options to override some settings in the used config."
msgstr ""

#: mmcls.apis.inference.init_model:12 of
msgid "The constructed classifier."
msgstr ""

#: mmcls.apis.train.init_random_seed:1 of
msgid "Initialize random seed."
msgstr ""

#: mmcls.apis.train.init_random_seed:3 of
msgid ""
"If the seed is not set, the seed will be automatically randomized, and "
"then broadcast to all processes to prevent some potential bugs."
msgstr ""

#: mmcls.apis.train.init_random_seed:6 of
msgid "The seed. Default to None."
msgstr ""

#: mmcls.apis.train.init_random_seed:8 of
msgid "The device where the seed will be put on. Default to 'cuda'."
msgstr ""

#: mmcls.apis.train.init_random_seed:12 mmcls.apis.train.set_random_seed:3 of
msgid "Seed to be used."
msgstr ""

#: mmcls.apis.test.multi_gpu_test:1 of
msgid "Test model with multiple gpus."
msgstr ""

#: mmcls.apis.test.multi_gpu_test:3 of
msgid ""
"This method tests model with multiple gpus and collects the results under"
" two different modes: gpu and cpu modes. By setting 'gpu_collect=True' it"
" encodes results to gpu tensors and use gpu communication for results "
"collection. On cpu mode it saves the results on different gpus to "
"'tmpdir' and collects them by the rank 0 worker."
msgstr ""

#: mmcls.apis.test.multi_gpu_test:9 of
msgid "Model to be tested."
msgstr ""

#: mmcls.apis.test.multi_gpu_test:11 of
msgid "Pytorch data loader."
msgstr ""

#: mmcls.apis.test.multi_gpu_test:13 of
msgid ""
"Path of directory to save the temporary results from different gpus under"
" cpu mode."
msgstr ""

#: mmcls.apis.test.multi_gpu_test:16 of
msgid "Option to use either gpu or cpu to collect results."
msgstr ""

#: mmcls.apis.test.multi_gpu_test:19 of
msgid "The prediction results."
msgstr ""

#: mmcls.apis.train.set_random_seed:1 of
msgid "Set random seed."
msgstr ""

#: mmcls.apis.train.set_random_seed:5 of
msgid ""
"Whether to set the deterministic option for CUDNN backend, i.e., set "
"`torch.backends.cudnn.deterministic` to True and "
"`torch.backends.cudnn.benchmark` to False. Default: False."
msgstr ""

#: mmcls.apis.inference.show_result_pyplot:1 of
msgid "Visualize the classification results on the image."
msgstr ""

#: mmcls.apis.inference.show_result_pyplot:5 of
msgid "Image filename or loaded image."
msgstr ""

#: mmcls.apis.inference.show_result_pyplot:7 of
msgid "The classification result."
msgstr ""

#: mmcls.apis.inference.show_result_pyplot:9 of
msgid "Figure size of the pyplot figure. Defaults to (15, 10)."
msgstr ""

#: mmcls.apis.inference.show_result_pyplot:12 of
msgid "Title of the pyplot figure. Defaults to 'result'."
msgstr ""

#: mmcls.apis.inference.show_result_pyplot:15
#: mmcls.models.classifiers.base.BaseClassifier.show_result:20 of
msgid "How many seconds to display the image. Defaults to 0."
msgstr ""

#: ../../api.rst:7
msgid "mmcls.core"
msgstr ""

#: ../../api.rst:10
msgid "evaluation"
msgstr ""

#: mmcls.core.evaluation.eval_hooks.DistEvalHook:1 of
msgid "Distributed evaluation hook."
msgstr ""

#: mmcls.core.evaluation.eval_hooks.DistEvalHook:3
#: mmcls.core.evaluation.eval_hooks.EvalHook:3
#: mmcls.datasets.builder.build_dataloader:35 of
msgid "A PyTorch dataloader."
msgstr ""

#: mmcls.core.evaluation.eval_hooks.DistEvalHook:5
#: mmcls.core.evaluation.eval_hooks.EvalHook:5 of
msgid "Evaluation interval (by epochs). Default: 1."
msgstr ""

#: mmcls.core.evaluation.eval_hooks.DistEvalHook:7 of
msgid "Temporary directory to save the results of all processes. Default: None."
msgstr ""

#: mmcls.core.evaluation.eval_hooks.DistEvalHook:10 of
msgid "Whether to use gpu or cpu to collect results. Default: False."
msgstr ""

#: mmcls.core.evaluation.eval_hooks.EvalHook:1 of
msgid "Evaluation hook."
msgstr ""

#: mmcls.core.evaluation.multilabel_eval_metrics.average_performance:1 of
msgid ""
"Calculate CP, CR, CF1, OP, OR, OF1, where C stands for per-class average,"
" O stands for overall average, P stands for precision, R stands for "
"recall and F1 stands for F1-score."
msgstr ""

#: mmcls.core.evaluation.mean_ap.mAP:3
#: mmcls.core.evaluation.multilabel_eval_metrics.average_performance:5 of
msgid "The model prediction with shape (N, C), where C is the number of classes."
msgstr ""

#: mmcls.core.evaluation.mean_ap.mAP:6
#: mmcls.core.evaluation.multilabel_eval_metrics.average_performance:8 of
msgid ""
"The target of each prediction with shape (N, C), where C is the number of"
" classes. 1 stands for positive examples, 0 stands for negative examples "
"and -1 stands for difficult examples."
msgstr ""

#: mmcls.core.evaluation.multilabel_eval_metrics.average_performance:13 of
msgid "The confidence threshold. Defaults to None."
msgstr ""

#: mmcls.core.evaluation.multilabel_eval_metrics.average_performance:15 of
msgid ""
"Top-k performance. Note that if thr and k are both given, k will be "
"ignored. Defaults to None."
msgstr ""

#: mmcls.core.evaluation.multilabel_eval_metrics.average_performance:19 of
msgid "(CP, CR, CF1, OP, OR, OF1)"
msgstr ""

#: mmcls.core.evaluation.mean_ap.average_precision:1 of
msgid "Calculate the average precision for a single class."
msgstr ""

#: mmcls.core.evaluation.mean_ap.average_precision:3 of
msgid ""
"AP summarizes a precision-recall curve as the weighted mean of maximum "
"precisions obtained for any r'>r, where r is the recall:"
msgstr ""

#: mmcls.core.evaluation.mean_ap.average_precision:6 of
msgid ""
"\\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\n"
"\n"
msgstr ""

#: mmcls.core.evaluation.mean_ap.average_precision:9 of
msgid ""
"Note that no approximation is involved since the curve is piecewise "
"constant."
msgstr ""

#: mmcls.core.evaluation.mean_ap.average_precision:12 of
msgid "The model prediction with shape (N, )."
msgstr ""

#: mmcls.core.evaluation.mean_ap.average_precision:14 of
msgid "The target of each prediction with shape (N, )."
msgstr ""

#: mmcls.core.evaluation.mean_ap.average_precision:17 of
msgid "a single float as average precision value."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.calculate_confusion_matrix:1 of
msgid "Calculate confusion matrix according to the prediction and target."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.calculate_confusion_matrix:3
#: mmcls.core.evaluation.eval_metrics.f1_score:3
#: mmcls.core.evaluation.eval_metrics.precision:3
#: mmcls.core.evaluation.eval_metrics.precision_recall_f1:4
#: mmcls.core.evaluation.eval_metrics.recall:3
#: mmcls.core.evaluation.eval_metrics.support:4 of
msgid "The model prediction with shape (N, C)."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.calculate_confusion_matrix:5
#: mmcls.core.evaluation.eval_metrics.f1_score:5
#: mmcls.core.evaluation.eval_metrics.precision:5
#: mmcls.core.evaluation.eval_metrics.precision_recall_f1:6
#: mmcls.core.evaluation.eval_metrics.recall:5
#: mmcls.core.evaluation.eval_metrics.support:6 of
msgid "The target of each prediction with shape (N, 1) or (N,)."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.calculate_confusion_matrix:9 of
msgid ""
"Confusion matrix     The shape is (C, C), where C is the number of "
"classes."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.calculate_confusion_matrix:11 of
msgid "Confusion matrix"
msgstr ""

#: mmcls.core.evaluation.eval_metrics.calculate_confusion_matrix:12 of
msgid "The shape is (C, C), where C is the number of classes."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.f1_score:1 of
msgid "Calculate F1 score according to the prediction and target."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.f1_score:8
#: mmcls.core.evaluation.eval_metrics.precision:8
#: mmcls.core.evaluation.eval_metrics.precision_recall_f1:9
#: mmcls.core.evaluation.eval_metrics.recall:8 of
msgid ""
"The type of averaging performed on the result. Options are 'macro' and "
"'none'. If 'none', the scores for each class are returned. If 'macro', "
"calculate metrics for each class, and find their unweighted mean. "
"Defaults to 'macro'."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.f1_score:14
#: mmcls.core.evaluation.eval_metrics.precision:14
#: mmcls.core.evaluation.eval_metrics.precision_recall_f1:15
#: mmcls.core.evaluation.eval_metrics.recall:14
#: mmcls.models.losses.accuracy.accuracy:11 of
msgid ""
"Predictions with scores under the thresholds are considered negative. "
"Default to 0."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.f1_score:18 of
msgid "F1 score."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.f1_score:19
#: mmcls.core.evaluation.eval_metrics.precision:19
#: mmcls.core.evaluation.eval_metrics.recall:19 of
msgid ""
"float | np.array | list[float | np.array]  "
"+----------------------------+--------------------+-------------------+ |"
" Args                       | ``thrs`` is number | ``thrs`` is tuple | "
"+============================+====================+===================+ |"
" ``average_mode`` = \"macro\" | float              | list[float]       | "
"+----------------------------+--------------------+-------------------+ |"
" ``average_mode`` = \"none\"  | np.array           | list[np.array]    | "
"+----------------------------+--------------------+-------------------+"
msgstr ""

#: mmcls.core.evaluation.eval_metrics.f1_score:19
#: mmcls.core.evaluation.eval_metrics.precision:19
#: mmcls.core.evaluation.eval_metrics.recall:19 of
msgid "float | np.array | list[float | np.array]"
msgstr ""

#: mmcls.core.evaluation.eval_metrics.f1_score:22
#: mmcls.core.evaluation.eval_metrics.precision:22
#: mmcls.core.evaluation.eval_metrics.precision_recall_f1:26
#: mmcls.core.evaluation.eval_metrics.recall:22 of
msgid "Args"
msgstr ""

#: mmcls.core.evaluation.eval_metrics.f1_score:22
#: mmcls.core.evaluation.eval_metrics.precision:22
#: mmcls.core.evaluation.eval_metrics.precision_recall_f1:26
#: mmcls.core.evaluation.eval_metrics.recall:22 of
msgid "``thrs`` is number"
msgstr ""

#: mmcls.core.evaluation.eval_metrics.f1_score:22
#: mmcls.core.evaluation.eval_metrics.precision:22
#: mmcls.core.evaluation.eval_metrics.precision_recall_f1:26
#: mmcls.core.evaluation.eval_metrics.recall:22 of
msgid "``thrs`` is tuple"
msgstr ""

#: mmcls.core.evaluation.eval_metrics.f1_score:24
#: mmcls.core.evaluation.eval_metrics.precision:24
#: mmcls.core.evaluation.eval_metrics.precision_recall_f1:28
#: mmcls.core.evaluation.eval_metrics.recall:24 of
msgid "``average_mode`` = \"macro\""
msgstr ""

#: mmcls.core.evaluation.eval_metrics.f1_score:24
#: mmcls.core.evaluation.eval_metrics.precision:24
#: mmcls.core.evaluation.eval_metrics.precision_recall_f1:28
#: mmcls.core.evaluation.eval_metrics.recall:24 of
msgid "float"
msgstr ""

#: mmcls.core.evaluation.eval_metrics.f1_score:24
#: mmcls.core.evaluation.eval_metrics.precision:24
#: mmcls.core.evaluation.eval_metrics.precision_recall_f1:28
#: mmcls.core.evaluation.eval_metrics.recall:24 of
msgid "list[float]"
msgstr ""

#: mmcls.core.evaluation.eval_metrics.f1_score:26
#: mmcls.core.evaluation.eval_metrics.precision:26
#: mmcls.core.evaluation.eval_metrics.precision_recall_f1:30
#: mmcls.core.evaluation.eval_metrics.recall:26 of
msgid "``average_mode`` = \"none\""
msgstr ""

#: mmcls.core.evaluation.eval_metrics.f1_score:26
#: mmcls.core.evaluation.eval_metrics.precision:26
#: mmcls.core.evaluation.eval_metrics.precision_recall_f1:30
#: mmcls.core.evaluation.eval_metrics.recall:26 of
msgid "np.array"
msgstr ""

#: mmcls.core.evaluation.eval_metrics.f1_score:26
#: mmcls.core.evaluation.eval_metrics.precision:26
#: mmcls.core.evaluation.eval_metrics.precision_recall_f1:30
#: mmcls.core.evaluation.eval_metrics.recall:26 of
msgid "list[np.array]"
msgstr ""

#: mmcls.core.evaluation.mean_ap.mAP:1 of
msgid "Calculate the mean average precision with respect of classes."
msgstr ""

#: mmcls.core.evaluation.mean_ap.mAP:12 of
msgid "A single float as mAP value."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.precision:1 of
msgid "Calculate precision according to the prediction and target."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.precision:18 of
msgid "Precision."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.precision_recall_f1:1 of
msgid ""
"Calculate precision, recall and f1 score according to the prediction and "
"target."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.precision_recall_f1:19 of
msgid ""
"tuple containing precision, recall, f1 score.      The type of precision,"
" recall, f1 score is one of the following:  "
"+----------------------------+--------------------+-------------------+ |"
" Args                       | ``thrs`` is number | ``thrs`` is tuple | "
"+============================+====================+===================+ |"
" ``average_mode`` = \"macro\" | float              | list[float]       | "
"+----------------------------+--------------------+-------------------+ |"
" ``average_mode`` = \"none\"  | np.array           | list[np.array]    | "
"+----------------------------+--------------------+-------------------+"
msgstr ""

#: mmcls.core.evaluation.eval_metrics.precision_recall_f1:21 of
msgid "tuple containing precision, recall, f1 score."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.precision_recall_f1:23 of
msgid "The type of precision, recall, f1 score is one of the following:"
msgstr ""

#: mmcls.core.evaluation.eval_metrics.recall:1 of
msgid "Calculate recall according to the prediction and target."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.recall:18 of
msgid "Recall."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.support:1 of
msgid ""
"Calculate the total number of occurrences of each label according to the "
"prediction and target."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.support:9 of
msgid ""
"The type of averaging performed on the result. Options are 'macro' and "
"'none'. If 'none', the scores for each class are returned. If 'macro', "
"calculate metrics for each class, and find their unweighted sum. Defaults"
" to 'macro'."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.support:16 of
msgid ""
"Support.      - If the ``average_mode`` is set to macro, the function "
"returns       a single float.     - If the ``average_mode`` is set to "
"none, the function returns       a np.array with shape C."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.support:18 of
msgid "Support."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.support:20 of
msgid ""
"If the ``average_mode`` is set to macro, the function returns a single "
"float."
msgstr ""

#: mmcls.core.evaluation.eval_metrics.support:22 of
msgid ""
"If the ``average_mode`` is set to none, the function returns a np.array "
"with shape C."
msgstr ""

#: ../../api.rst:15
msgid "mmcls.models"
msgstr ""

#: ../../api.rst:18
msgid "models"
msgstr ""

#: mmcls.models.builder.build_backbone:1 of
msgid "Build backbone."
msgstr ""

#: mmcls.models.builder.build_head:1 of
msgid "Build head."
msgstr ""

#: mmcls.models.builder.build_loss:1 of
msgid "Build loss."
msgstr ""

#: mmcls.models.builder.build_neck:1 of
msgid "Build neck."
msgstr ""

#: ../../api.rst:23
msgid "classifiers"
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier:1 of
msgid "Base class for classifiers."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward:1 of
msgid ""
"Calls either forward_train or forward_test depending on whether "
"return_loss=True."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward:4 of
msgid ""
"Note this setting will change the expected inputs. When "
"`return_loss=True`, img and img_meta are single-nested (i.e. Tensor and "
"List[dict]), and when `resturn_loss=False`, img and img_meta should be "
"double nested (i.e.  List[Tensor], List[List[dict]]), with the outer list"
" indicating test time augmentations."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward_test:1 of
msgid ""
"the outer list indicates test-time augmentations and inner Tensor should "
"have a shape NxCxHxW, which contains all images in the batch."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward_train:1 of
msgid ""
"List of tensors of shape (1, C, H, W). Typically these should be mean "
"centered and std scaled."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.forward_train:4 of
msgid "Specific to concrete implementation."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.show_result:1 of
msgid "Draw `result` over `img`."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.show_result:3 of
msgid "The image to be displayed."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.show_result:5 of
msgid "The classification results to draw over `img`."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.show_result:7 of
msgid "Color of texts."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.show_result:9 of
msgid "Font scales of texts."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.show_result:11 of
msgid "width between each row of results on the image."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.show_result:13 of
msgid "Whether to show the image. Default: False."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.show_result:16 of
msgid "Image show figure size. Defaults to (15, 10)."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.show_result:18 of
msgid "The window name."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.show_result:23 of
msgid "The filename to write the image. Default: None."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.show_result:27 of
msgid "Image with overlaid results."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.train_step:1 of
msgid "The iteration step during training."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.train_step:3 of
msgid ""
"This method defines an iteration step during training, except for the "
"back propagation and optimizer updating, which are done in an optimizer "
"hook. Note that in some complicated cases or models, the whole process "
"including back propagation and optimizer updating are also defined in "
"this method, such as GAN."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.train_step:9
#: mmcls.models.classifiers.base.BaseClassifier.val_step:7 of
msgid "The output of dataloader."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.train_step:11
#: mmcls.models.classifiers.base.BaseClassifier.val_step:9 of
msgid ""
"The optimizer of runner is passed to ``train_step()``. This argument is "
"unused and reserved."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.train_step:16
#: mmcls.models.classifiers.base.BaseClassifier.val_step:14 of
msgid ""
"Dict of outputs. The following fields are contained.     - loss "
"(torch.Tensor): A tensor for back propagation, which                     "
"can be a weighted sum of multiple losses.     - log_vars (dict): Dict "
"contains all the variables to be sent                     to the logger."
"     - num_samples (int): Indicates the batch size (when the model"
"                     is DDP, it means the batch size on each GPU), which "
"is                     used for averaging the logs."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.train_step:20
#: mmcls.models.classifiers.base.BaseClassifier.val_step:18 of
msgid "Dict of outputs. The following fields are contained."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.train_step:19
#: mmcls.models.classifiers.base.BaseClassifier.val_step:17 of
msgid ""
"loss (torch.Tensor): A tensor for back propagation, which"
"                     can be a weighted sum of multiple losses."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.train_step:20
#: mmcls.models.classifiers.base.BaseClassifier.val_step:18 of
msgid ""
"log_vars (dict): Dict contains all the variables to be sent"
"                     to the logger."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.train_step:21
#: mmcls.models.classifiers.base.BaseClassifier.val_step:19 of
msgid ""
"num_samples (int): Indicates the batch size (when the model"
"                     is DDP, it means the batch size on each GPU), which "
"is                     used for averaging the logs."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.val_step:1 of
msgid "The iteration step during validation."
msgstr ""

#: mmcls.models.classifiers.base.BaseClassifier.val_step:3 of
msgid ""
"This method shares the same signature as :func:`train_step`, but used "
"during val epochs. Note that the evaluation after training epochs is not "
"implemented with this method, but an evaluation hook."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.extract_feat:1 of
msgid "Directly extract features from the backbone + neck."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.forward_train:1 of
msgid "Forward computation during training."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.forward_train:3 of
msgid ""
"of shape (N, C, H, W) encoding input images. Typically these should be "
"mean centered and std scaled."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.forward_train:6 of
msgid ""
"It should be of shape (N, 1) encoding the ground-truth label of input "
"images for single label task. It shoulf be of shape (N, C) encoding the "
"ground-truth label of input images for multi-labels task."
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.forward_train:12 of
msgid "a dictionary of loss components"
msgstr ""

#: mmcls.models.classifiers.image.ImageClassifier.simple_test:1
#: mmcls.models.heads.cls_head.ClsHead.simple_test:1
#: mmcls.models.heads.conformer_head.ConformerHead.simple_test:1
#: mmcls.models.heads.linear_head.LinearClsHead.simple_test:1
#: mmcls.models.heads.multi_label_linear_head.MultiLabelLinearClsHead.simple_test:1
#: mmcls.models.heads.stacked_head.StackedLinearClsHead.simple_test:1
#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead.simple_test:1
#: of
msgid "Test without augmentation."
msgstr ""

#: ../../api.rst:28
msgid "backbones"
msgstr ""

#: mmcls.models.backbones.alexnet.AlexNet:1 of
msgid "`AlexNet <https://en.wikipedia.org/wiki/AlexNet>`_ backbone."
msgstr ""

#: mmcls.models.backbones.alexnet.AlexNet:3 of
msgid "The input for AlexNet is a 224x224 RGB image."
msgstr ""

#: mmcls.models.backbones.alexnet.AlexNet:5
#: mmcls.models.backbones.lenet.LeNet5:5 of
msgid ""
"number of classes for classification. The default value is -1, which uses"
" the backbone as a feature extractor without the top classifier."
msgstr ""

#: mmcls.models.backbones.alexnet.AlexNet.forward:1
#: mmcls.models.backbones.conformer.Conformer.forward:1
#: mmcls.models.backbones.lenet.LeNet5.forward:1
#: mmcls.models.backbones.mlp_mixer.MlpMixer.forward:1
#: mmcls.models.backbones.mobilenet_v2.MobileNetV2.forward:1
#: mmcls.models.backbones.mobilenet_v3.MobileNetV3.forward:1
#: mmcls.models.backbones.regnet.RegNet.forward:1
#: mmcls.models.backbones.repvgg.RepVGG.forward:1
#: mmcls.models.backbones.resnet.ResNet.forward:1
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR.forward:1
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1.forward:1
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2.forward:1
#: mmcls.models.backbones.swin_transformer.SwinTransformer.forward:1
#: mmcls.models.backbones.t2t_vit.T2T_ViT.forward:1
#: mmcls.models.backbones.timm_backbone.TIMMBackbone.forward:1
#: mmcls.models.backbones.tnt.TNT.forward:1
#: mmcls.models.backbones.vgg.VGG.forward:1
#: mmcls.models.backbones.vision_transformer.VisionTransformer.forward:1 of
msgid "Forward computation."
msgstr ""

#: mmcls.models.backbones.alexnet.AlexNet.forward:3
#: mmcls.models.backbones.conformer.Conformer.forward:3
#: mmcls.models.backbones.lenet.LeNet5.forward:3
#: mmcls.models.backbones.mlp_mixer.MlpMixer.forward:3
#: mmcls.models.backbones.mobilenet_v2.MobileNetV2.forward:3
#: mmcls.models.backbones.mobilenet_v3.MobileNetV3.forward:3
#: mmcls.models.backbones.regnet.RegNet.forward:3
#: mmcls.models.backbones.repvgg.RepVGG.forward:3
#: mmcls.models.backbones.resnet.ResNet.forward:3
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR.forward:3
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1.forward:3
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2.forward:3
#: mmcls.models.backbones.swin_transformer.SwinTransformer.forward:3
#: mmcls.models.backbones.t2t_vit.T2T_ViT.forward:3
#: mmcls.models.backbones.timm_backbone.TIMMBackbone.forward:3
#: mmcls.models.backbones.tnt.TNT.forward:3
#: mmcls.models.backbones.vgg.VGG.forward:3
#: mmcls.models.backbones.vision_transformer.VisionTransformer.forward:3 of
msgid ""
"x could be a Torch.tensor or a tuple of Torch.tensor, containing input "
"data for forward computation."
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:1 of
msgid "Conformer backbone."
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:3 of
msgid ""
"A PyTorch implementation of : `Conformer: Local Features Coupling Global "
"Representations for Visual Recognition "
"<https://arxiv.org/abs/2105.03889>`_"
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:6 of
msgid "Conformer architecture. Defaults to 'tiny'."
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:8 of
msgid "The patch size. Defaults to 16."
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:10 of
msgid "The base number of channels in CNN network. Defaults to 64."
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:13 of
msgid "The expansion ratio of FFN network in transformer block. Defaults to 4."
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:16 of
msgid "Whether use class token or not. Defaults to True."
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:19
#: mmcls.models.backbones.mlp_mixer.MlpMixer:19
#: mmcls.models.backbones.t2t_vit.T2T_ViT:27
#: mmcls.models.backbones.vision_transformer.VisionTransformer:20 of
msgid "stochastic depth rate. Defaults to 0."
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:21
#: mmcls.models.backbones.t2t_vit.T2T_ViT:21
#: mmcls.models.backbones.vision_transformer.VisionTransformer:14 of
msgid "Output from which stages. Defaults to -1, means the last stage."
msgstr ""

#: mmcls.models.backbones.conformer.Conformer:24
#: mmcls.models.backbones.mlp_mixer.MlpMixer:31
#: mmcls.models.backbones.res2net.Res2Net:51
#: mmcls.models.backbones.vision_transformer.VisionTransformer:39 of
msgid "Initialization config dict. Defaults to None."
msgstr ""

#: mmcls.models.backbones.conformer.Conformer.init_weights:1
#: mmcls.models.backbones.resnet.ResNet.init_weights:1
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1.init_weights:1
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2.init_weights:1
#: mmcls.models.backbones.swin_transformer.SwinTransformer.init_weights:1
#: mmcls.models.backbones.t2t_vit.T2T_ViT.init_weights:1
#: mmcls.models.backbones.vision_transformer.VisionTransformer.init_weights:1
#: mmcls.models.heads.conformer_head.ConformerHead.init_weights:1
#: mmcls.models.heads.stacked_head.StackedLinearClsHead.init_weights:1
#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead.init_weights:1
#: of
msgid "Initialize the weights."
msgstr ""

#: mmcls.models.backbones.lenet.LeNet5:1 of
msgid "`LeNet5 <https://en.wikipedia.org/wiki/LeNet>`_ backbone."
msgstr ""

#: mmcls.models.backbones.lenet.LeNet5:3 of
msgid "The input for LeNet-5 is a 32×32 grayscale image."
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:1 of
msgid "Mlp-Mixer backbone."
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:3 of
msgid ""
"Pytorch implementation of `MLP-Mixer: An all-MLP Architecture for Vision "
"<https://arxiv.org/pdf/2105.01601.pdf>`_"
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:6 of
msgid "MLP Mixer architecture Defaults to 'b'."
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:9
#: mmcls.models.backbones.t2t_vit.T2T_ViT:6 of
msgid "Input image size."
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:11 of
msgid "The patch size."
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:13 of
msgid "Output from which layer. Defaults to -1, means the last layer."
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:16
#: mmcls.models.backbones.vision_transformer.VisionTransformer:17 of
msgid "Probability of an element to be zeroed. Defaults to 0."
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:21
#: mmcls.models.backbones.t2t_vit.T2T_ViT:29
#: mmcls.models.backbones.vision_transformer.VisionTransformer:22 of
msgid "Config dict for normalization layer. Defaults to ``dict(type='LN')``."
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:24 of
msgid "The activation config for FFNs. Default GELU."
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:26
#: mmcls.models.backbones.vision_transformer.VisionTransformer:34 of
msgid "Configs of patch embeding. Defaults to an empty dict."
msgstr ""

#: mmcls.models.backbones.mlp_mixer.MlpMixer:28 of
msgid "Configs of each mixer block layer. Defaults to an empty dict."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2:1 of
msgid "MobileNetV2 backbone."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2:3 of
msgid ""
"Width multiplier, multiply number of channels in each layer by this "
"amount. Default: 1.0."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2:6 of
msgid "Output from which stages. Default: (7, )."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2:9
#: mmcls.models.backbones.mobilenet_v3.MobileNetV3:15
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:12
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2:9 of
msgid ""
"Stages to be frozen (all param fixed). Default: -1, which means not "
"freezing any parameters."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2:12
#: mmcls.models.backbones.mobilenet_v3.MobileNetV3:6
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:15
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2:12
#: mmcls.models.utils.inverted_residual.InvertedResidual:17
#: mmcls.models.utils.se_layer.SELayer:16 of
msgid ""
"Config dict for convolution layer. Default: None, which means using "
"conv2d."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2:15
#: mmcls.models.backbones.mobilenet_v3.MobileNetV3:9
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:18
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2:15
#: mmcls.models.utils.inverted_residual.InvertedResidual:20 of
msgid "Config dict for normalization layer. Default: dict(type='BN')."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2:18 of
msgid "Config dict for activation layer. Default: dict(type='ReLU6')."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2:21
#: mmcls.models.backbones.mobilenet_v3.MobileNetV3:18
#: mmcls.models.backbones.regnet.RegNet:33
#: mmcls.models.backbones.repvgg.RepVGG:45
#: mmcls.models.backbones.resnest.ResNeSt:55
#: mmcls.models.backbones.resnet.ResNet:42
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:46
#: mmcls.models.backbones.resnext.ResNeXt:47
#: mmcls.models.backbones.seresnet.SEResNet:44
#: mmcls.models.backbones.seresnext.SEResNeXt:49
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:24
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2:21
#: mmcls.models.backbones.vgg.VGG:23 of
msgid ""
"Whether to set norm layers to eval mode, namely, freeze running stats "
"(mean and var). Note: Effect on Batch Norm and its variants only. "
"Default: False."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2:25
#: mmcls.models.backbones.mobilenet_v3.MobileNetV3:22
#: mmcls.models.backbones.regnet.RegNet:37
#: mmcls.models.backbones.repvgg.RepVGG:39
#: mmcls.models.backbones.resnest.ResNeSt:59
#: mmcls.models.backbones.resnet.ResNet:46
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:50
#: mmcls.models.backbones.resnext.ResNeXt:51
#: mmcls.models.backbones.seresnet.SEResNet:48
#: mmcls.models.backbones.seresnext.SEResNeXt:53
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:28
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2:25
#: mmcls.models.utils.inverted_residual.InvertedResidual:26 of
msgid ""
"Use checkpoint or not. Using checkpoint will save some memory while "
"slowing down the training speed. Default: False."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2.make_layer:1 of
msgid "Stack InvertedResidual blocks to build a layer for MobileNetV2."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2.make_layer:3 of
msgid "out_channels of block."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2.make_layer:5 of
msgid "number of blocks."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2.make_layer:7 of
msgid "stride of the first block. Default: 1"
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2.make_layer:9 of
msgid ""
"Expand the number of channels of the hidden layer in InvertedResidual by "
"this ratio. Default: 6."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2.train:1
#: mmcls.models.backbones.mobilenet_v3.MobileNetV3.train:1
#: mmcls.models.backbones.repvgg.RepVGG.train:1
#: mmcls.models.backbones.resnet.ResNet.train:1
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1.train:1
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2.train:1
#: mmcls.models.backbones.swin_transformer.SwinTransformer.train:1
#: mmcls.models.backbones.vgg.VGG.train:1 of
msgid "Set module status before forward computation."
msgstr ""

#: mmcls.models.backbones.mobilenet_v2.MobileNetV2.train:3
#: mmcls.models.backbones.mobilenet_v3.MobileNetV3.train:3
#: mmcls.models.backbones.repvgg.RepVGG.train:3
#: mmcls.models.backbones.resnet.ResNet.train:3
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1.train:3
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2.train:3
#: mmcls.models.backbones.swin_transformer.SwinTransformer.train:3
#: mmcls.models.backbones.vgg.VGG.train:3 of
msgid "Whether it is train_mode or test_mode"
msgstr ""

#: mmcls.models.backbones.mobilenet_v3.MobileNetV3:1 of
msgid "MobileNetV3 backbone."
msgstr ""

#: mmcls.models.backbones.mobilenet_v3.MobileNetV3:3 of
msgid "Architecture of mobilnetv3, from {small, large}. Default: small."
msgstr ""

#: mmcls.models.backbones.mobilenet_v3.MobileNetV3:12 of
msgid ""
"Output from which stages. Default: None, which means output tensors from "
"final stage."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:1 of
msgid "RegNet backbone."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:3 of
msgid "More details can be found in `paper <https://arxiv.org/abs/2003.13678>`_ ."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:5 of
msgid ""
"The parameter of RegNets. - w0 (int): initial width - wa (float): slope "
"of width - wm (float): quantization parameter to quantize the width - "
"depth (int): depth of the backbone - group_w (int): width of group - "
"bot_mul (float): bottleneck ratio, i.e. expansion of bottleneck."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:13 of
msgid "Strides of the first block of each stage."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:15 of
msgid "Base channels after stem layer."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:17
#: mmcls.models.backbones.repvgg.RepVGG:15
#: mmcls.models.backbones.resnest.ResNeSt:21
#: mmcls.models.backbones.resnet.ResNet:8
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:10
#: mmcls.models.backbones.resnext.ResNeXt:13
#: mmcls.models.backbones.seresnet.SEResNet:10
#: mmcls.models.backbones.seresnext.SEResNeXt:15
#: mmcls.models.backbones.timm_backbone.TIMMBackbone:11 of
msgid "Number of input image channels. Default: 3."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:19 mmcls.models.backbones.vgg.VGG:11 of
msgid "Dilation of each stage."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:21 of
msgid "Output from which stages."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:23 of
msgid ""
"`pytorch` or `caffe`. If set to \"pytorch\", the stride-two layer is the "
"3x3 conv layer, otherwise the stride-two layer is the first 1x1 conv "
"layer. Default: \"pytorch\"."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:27
#: mmcls.models.backbones.repvgg.RepVGG:28 of
msgid ""
"Stages to be frozen (all param fixed). -1 means not freezing any "
"parameters. Default: -1."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:30 of
msgid ""
"dictionary to construct and config norm layer. Default: dict(type='BN', "
"requires_grad=True)."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:40 of
msgid ""
"whether to use zero init for last norm layer in resblocks to let them "
"behave as identity. Default: True."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet:45
#: mmcls.models.backbones.res2net.Res2Net:56
#: mmcls.models.backbones.resnet.ResNet:54
#: mmcls.models.backbones.seresnet.SEResNet:56
#: mmcls.models.utils.augment.augments.Augments:9 of
msgid "示例"
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.adjust_width_group:1 of
msgid "Adjusts the compatibility of widths and groups."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.adjust_width_group:3 of
msgid "Width of each stage."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.adjust_width_group:5 of
msgid "Bottleneck ratio."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.adjust_width_group:7 of
msgid "number of groups in each stage"
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.adjust_width_group:10 of
msgid "The adjusted widths and groups of each stage."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:1 of
msgid "Generates per block width from RegNet parameters."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:3 of
msgid "Initial width of the backbone"
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:5 of
msgid "Slope of the quantized linear function"
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:7 of
msgid "Parameter used to quantize the width."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:9 of
msgid "Depth of the backbone."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:11 of
msgid "The divisor of channels. Defaults to 8."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:14 of
msgid ""
"tuple containing:     - list: Widths of each stage.     - int: The number"
" of stages."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:17 of
msgid "tuple containing:"
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:17 of
msgid "list: Widths of each stage."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.generate_regnet:18 of
msgid "int: The number of stages."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.get_stages_from_blocks:1 of
msgid "Gets widths/stage_blocks of network at each stage."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.get_stages_from_blocks:3 of
msgid "Width in each stage."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.get_stages_from_blocks:6 of
msgid "width and depth of each stage"
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.quantize_float:1 of
msgid "Converts a float to closest non-zero int divisible by divior."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.quantize_float:3 of
msgid "Original number to be quantized."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.quantize_float:5 of
msgid "Divisor used to quantize the number."
msgstr ""

#: mmcls.models.backbones.regnet.RegNet.quantize_float:8 of
msgid "quantized number that is divisible by devisor."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:1 of
msgid "RepVGG backbone."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:3 of
msgid ""
"A PyTorch impl of : `RepVGG: Making VGG-style ConvNets Great Again "
"<https://arxiv.org/abs/2101.03697>`_"
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:6 of
msgid ""
"The parameter of RepVGG. If it's a dict, it should contain the following "
"keys:  - num_blocks (Sequence[int]): Number of blocks in each stage. - "
"width_factor (Sequence[float]): Width deflator in each stage. - "
"group_layer_map (dict | None): RepVGG Block that declares   the need to "
"apply group convolution. - se_cfg (dict | None): Se Layer config"
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:6 of
msgid ""
"The parameter of RepVGG. If it's a dict, it should contain the following "
"keys:"
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:9 of
msgid "num_blocks (Sequence[int]): Number of blocks in each stage."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:10 of
msgid "width_factor (Sequence[float]): Width deflator in each stage."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:11 of
msgid ""
"group_layer_map (dict | None): RepVGG Block that declares the need to "
"apply group convolution."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:13 of
msgid "se_cfg (dict | None): Se Layer config"
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:17 of
msgid ""
"Base channels of RepVGG backbone, work with width_factor together. "
"Default: 64."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:20 of
msgid "Output from which stages. Default: (3, )."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:22 of
msgid "Strides of the first block of each stage. Default: (2, 2, 2, 2)."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:25 of
msgid "Dilation of each stage. Default: (1, 1, 1, 1)."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:31
#: mmcls.models.backbones.resnest.ResNeSt:51
#: mmcls.models.backbones.resnet.ResNet:38
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:42
#: mmcls.models.backbones.resnext.ResNeXt:43
#: mmcls.models.backbones.seresnet.SEResNet:40
#: mmcls.models.backbones.seresnext.SEResNeXt:45
#: mmcls.models.utils.embed.HybridEmbed:17 of
msgid "The config dict for conv layers. Default: None."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:33 of
msgid "The config dict for norm layers. Default: dict(type='BN')."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:36
#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:21
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2:18
#: mmcls.models.utils.inverted_residual.InvertedResidual:23 of
msgid "Config dict for activation layer. Default: dict(type='ReLU')."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:42 of
msgid "Whether to switch the model structure to deployment mode. Default: False."
msgstr ""

#: mmcls.models.backbones.repvgg.RepVGG:49 of
msgid "Initialization config dict."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:1 of
msgid "Res2Net backbone."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:3 of
msgid ""
"A PyTorch implement of : `Res2Net: A New Multi-scale Backbone "
"Architecture <https://arxiv.org/pdf/1904.01169.pdf>`_"
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:6 of
msgid "Depth of Res2Net, choose from {50, 101, 152}."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:8 of
msgid "Scales used in Res2Net. Defaults to 4."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:10 of
msgid "Basic width of each scale. Defaults to 26."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:12 of
msgid "Number of input image channels. Defaults to 3."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:14 of
msgid "Number of Res2Net stages. Defaults to 4."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:16 of
msgid "Strides of the first block of each stage. Defaults to ``(1, 2, 2, 2)``."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:19 of
msgid "Dilation of each stage. Defaults to ``(1, 1, 1, 1)``."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:22 of
msgid "Output from which stages. Defaults to ``(3, )``."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:25 of
msgid ""
"\"pytorch\" or \"caffe\". If set to \"pytorch\", the stride-two layer is "
"the 3x3 conv layer, otherwise the stride-two layer is the first 1x1 conv "
"layer. Defaults to \"pytorch\"."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:29 of
msgid "Replace 7x7 conv in input stem with 3 3x3 conv. Defaults to True."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:32 of
msgid ""
"Use AvgPool instead of stride conv when downsampling in the bottle2neck. "
"Defaults to True."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:35
#: mmcls.models.backbones.swin_transformer.SwinTransformer:31 of
msgid ""
"Stages to be frozen (stop grad and set eval mode). -1 means not freezing "
"any parameters. Defaults to -1."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:38 of
msgid ""
"Dictionary to construct and config norm layer. Defaults to "
"``dict(type='BN', requires_grad=True)``."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:41
#: mmcls.models.backbones.swin_transformer.SwinTransformer:34 of
msgid ""
"Whether to set norm layers to eval mode, namely, freeze running stats "
"(mean and var). Note: Effect on Batch Norm and its variants only. "
"Defaults to False."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:45
#: mmcls.models.backbones.swin_transformer.SwinTransformer:27 of
msgid ""
"Use checkpoint or not. Using checkpoint will save some memory while "
"slowing down the training speed. Defaults to False."
msgstr ""

#: mmcls.models.backbones.res2net.Res2Net:48 of
msgid ""
"Whether to use zero init for last norm layer in resblocks to let them "
"behave as identity. Defaults to True."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:1 of
msgid "ResNeSt backbone."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:3 of
msgid ""
"Please refer to the `paper <https://arxiv.org/pdf/2004.08955.pdf>`__ for "
"details."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:6 of
msgid "Network depth, from {50, 101, 152, 200}."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:8
#: mmcls.models.backbones.resnext.ResNeXt:8
#: mmcls.models.backbones.seresnext.SEResNeXt:8 of
msgid "Groups of conv2 in Bottleneck. Default: 32."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:10
#: mmcls.models.backbones.resnext.ResNeXt:10
#: mmcls.models.backbones.seresnext.SEResNeXt:10 of
msgid "Width per group of conv2 in Bottleneck. Default: 4."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:13 of
msgid "Radix of SpltAtConv2d. Default: 2"
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:15 of
msgid "Reduction factor of SplitAttentionConv2d. Default: 4."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:18 of
msgid "Whether to use average pool for stride in Bottleneck. Default: True."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:23
#: mmcls.models.backbones.resnet.ResNet:10
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:12
#: mmcls.models.backbones.resnext.ResNeXt:15
#: mmcls.models.backbones.seresnet.SEResNet:12
#: mmcls.models.backbones.seresnext.SEResNeXt:17 of
msgid "Output channels of the stem layer. Default: 64."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:25
#: mmcls.models.backbones.resnet.ResNet:14
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:16
#: mmcls.models.backbones.resnext.ResNeXt:17
#: mmcls.models.backbones.seresnet.SEResNet:14
#: mmcls.models.backbones.seresnext.SEResNeXt:19 of
msgid "Stages of the network. Default: 4."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:27
#: mmcls.models.backbones.resnet.ResNet:16
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:18
#: mmcls.models.backbones.resnext.ResNeXt:19
#: mmcls.models.backbones.seresnet.SEResNet:16
#: mmcls.models.backbones.seresnext.SEResNeXt:21 of
msgid "Strides of the first block of each stage. Default: ``(1, 2, 2, 2)``."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:30
#: mmcls.models.backbones.resnet.ResNet:19
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:21
#: mmcls.models.backbones.resnext.ResNeXt:22
#: mmcls.models.backbones.seresnet.SEResNet:19
#: mmcls.models.backbones.seresnext.SEResNeXt:24 of
msgid "Dilation of each stage. Default: ``(1, 1, 1, 1)``."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:33
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:24
#: mmcls.models.backbones.resnext.ResNeXt:25
#: mmcls.models.backbones.seresnet.SEResNet:22
#: mmcls.models.backbones.seresnext.SEResNeXt:27 of
msgid ""
"Output from which stages. If only one stage is specified, a single tensor"
" (feature map) is returned, otherwise multiple stages are specified, a "
"tuple of tensors will be returned. Default: ``(3, )``."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:38
#: mmcls.models.backbones.resnet.ResNet:25
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:29
#: mmcls.models.backbones.resnext.ResNeXt:30
#: mmcls.models.backbones.seresnet.SEResNet:27
#: mmcls.models.backbones.seresnext.SEResNeXt:32 of
msgid ""
"`pytorch` or `caffe`. If set to \"pytorch\", the stride-two layer is the "
"3x3 conv layer, otherwise the stride-two layer is the first 1x1 conv "
"layer."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:42
#: mmcls.models.backbones.resnet.ResNet:29
#: mmcls.models.backbones.resnext.ResNeXt:34
#: mmcls.models.backbones.seresnet.SEResNet:31
#: mmcls.models.backbones.seresnext.SEResNeXt:36 of
msgid "Replace 7x7 conv in input stem with 3 3x3 conv. Default: False."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:45
#: mmcls.models.backbones.resnet.ResNet:32
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:36
#: mmcls.models.backbones.resnext.ResNeXt:37
#: mmcls.models.backbones.seresnet.SEResNet:34
#: mmcls.models.backbones.seresnext.SEResNeXt:39 of
msgid ""
"Use AvgPool instead of stride conv when downsampling in the bottleneck. "
"Default: False."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:48
#: mmcls.models.backbones.resnet.ResNet:35
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:39
#: mmcls.models.backbones.resnext.ResNeXt:40
#: mmcls.models.backbones.seresnet.SEResNet:37
#: mmcls.models.backbones.seresnext.SEResNeXt:42 of
msgid ""
"Stages to be frozen (stop grad and set eval mode). -1 means not freezing "
"any parameters. Default: -1."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:53
#: mmcls.models.backbones.resnet.ResNet:40
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:44
#: mmcls.models.backbones.resnext.ResNeXt:45
#: mmcls.models.backbones.seresnet.SEResNet:42
#: mmcls.models.backbones.seresnext.SEResNeXt:47 of
msgid "The config dict for norm layers."
msgstr ""

#: mmcls.models.backbones.resnest.ResNeSt:62
#: mmcls.models.backbones.resnet.ResNet:49
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:53
#: mmcls.models.backbones.resnext.ResNeXt:54
#: mmcls.models.backbones.seresnet.SEResNet:51
#: mmcls.models.backbones.seresnext.SEResNeXt:56 of
msgid ""
"Whether to use zero init for last norm layer in resblocks to let them "
"behave as identity. Default: True."
msgstr ""

#: mmcls.models.backbones.resnext.ResNeXt:1 of
msgid "ResNeXt backbone."
msgstr ""

#: mmcls.models.backbones.resnext.ResNeXt:3 of
msgid ""
"Please refer to the `paper <https://arxiv.org/abs/1611.05431>`__ for "
"details."
msgstr ""

#: mmcls.models.backbones.resnext.ResNeXt:6
#: mmcls.models.backbones.seresnet.SEResNet:6
#: mmcls.models.backbones.seresnext.SEResNeXt:6 of
msgid "Network depth, from {50, 101, 152}."
msgstr ""

#: mmcls.models.backbones.resnet.ResNet:1 of
msgid "ResNet backbone."
msgstr ""

#: mmcls.models.backbones.resnet.ResNet:3 of
msgid ""
"Please refer to the `paper <https://arxiv.org/abs/1512.03385>`__ for "
"details."
msgstr ""

#: mmcls.models.backbones.resnet.ResNet:6
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:8 of
msgid "Network depth, from {18, 34, 50, 101, 152}."
msgstr ""

#: mmcls.models.backbones.resnet.ResNet:12
#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:14 of
msgid "Middle channels of the first stage. Default: 64."
msgstr ""

#: mmcls.models.backbones.resnet.ResNet:22 of
msgid "Output from which stages. Default: ``(3, )``."
msgstr ""

#: mmcls.models.backbones.resnet.ResNetV1d:1 of
msgid "ResNetV1d backbone."
msgstr ""

#: mmcls.models.backbones.resnet.ResNetV1d:3 of
msgid ""
"This variant is described in `Bag of Tricks. "
"<https://arxiv.org/pdf/1812.01187.pdf>`_."
msgstr ""

#: mmcls.models.backbones.resnet.ResNetV1d:6 of
msgid ""
"Compared with default ResNet(ResNetV1b), ResNetV1d replaces the 7x7 conv "
"in the input stem with three 3x3 convs. And in the downsampling block, a "
"2x2 avg_pool with stride 2 is added before conv, whose stride is changed "
"to 1."
msgstr ""

#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:1 of
msgid "ResNet backbone for CIFAR."
msgstr ""

#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:3 of
msgid ""
"Compared to standard ResNet, it uses `kernel_size=3` and `stride=1` in "
"conv1, and does not apply MaxPoolinng after stem. It has been proven to "
"be more efficient than standard ResNet in other public codebase, e.g., "
"`https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py`."
msgstr ""

#: mmcls.models.backbones.resnet_cifar.ResNet_CIFAR:33 of
msgid "This network has specific designed stem, thus it is asserted to be False."
msgstr ""

#: mmcls.models.backbones.seresnext.SEResNeXt:1 of
msgid "SEResNeXt backbone."
msgstr ""

#: mmcls.models.backbones.seresnet.SEResNet:3
#: mmcls.models.backbones.seresnext.SEResNeXt:3 of
msgid ""
"Please refer to the `paper <https://arxiv.org/abs/1709.01507>`__ for "
"details."
msgstr ""

#: mmcls.models.backbones.seresnet.SEResNet:8
#: mmcls.models.backbones.seresnext.SEResNeXt:13 of
msgid "Squeeze ratio in SELayer. Default: 16."
msgstr ""

#: mmcls.models.backbones.seresnet.SEResNet:1 of
msgid "SEResNet backbone."
msgstr ""

#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:1 of
msgid "ShuffleNetV1 backbone."
msgstr ""

#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:3 of
msgid ""
"The number of groups to be used in grouped 1x1 convolutions in each "
"ShuffleUnit. Default: 3."
msgstr ""

#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:6
#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2:3 of
msgid ""
"Width multiplier - adjusts the number of channels in each layer by this "
"amount. Default: 1.0."
msgstr ""

#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1:9 of
msgid "Output from which stages. Default: (2, )"
msgstr ""

#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1.make_layer:1 of
msgid "Stack ShuffleUnit blocks to make a layer."
msgstr ""

#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1.make_layer:3 of
msgid "out_channels of the block."
msgstr ""

#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1.make_layer:5 of
msgid "Number of blocks."
msgstr ""

#: mmcls.models.backbones.shufflenet_v1.ShuffleNetV1.make_layer:7 of
msgid ""
"Whether is the first ShuffleUnit of a sequential ShuffleUnits. Default: "
"False, which means using the grouped 1x1 convolution."
msgstr ""

#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2:1 of
msgid "ShuffleNetV2 backbone."
msgstr ""

#: mmcls.models.backbones.shufflenet_v2.ShuffleNetV2:6 of
msgid "Output from which stages. Default: (0, 1, 2, 3)."
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:1 of
msgid ""
"Swin Transformer A PyTorch implement of : `Swin Transformer: Hierarchical"
" Vision Transformer using Shifted Windows "
"<https://arxiv.org/abs/2103.14030>`_"
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:6 of
msgid "Inspiration from https://github.com/microsoft/Swin-Transformer"
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:9 of
msgid "Swin Transformer architecture Defaults to 'T'."
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:12 of
msgid "The size of input image. Defaults to 224."
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:15 of
msgid "The num of input channels. Defaults to 3."
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:18 of
msgid "Dropout rate after embedding. Defaults to 0."
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:21 of
msgid "Stochastic depth rate. Defaults to 0.1."
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:24 of
msgid ""
"If True, add absolute position embedding to the patch embedding. Defaults"
" to False."
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:38 of
msgid "If True, auto pad feature map to fit window_size. Defaults to False."
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:41 of
msgid ""
"Config dict for normalization layer at end of backone. Defaults to "
"dict(type='LN')"
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:44 of
msgid "Extra config dict for each stage. Defaults to empty dict."
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:47 of
msgid "Extra config dict for patch embedding. Defaults to empty dict."
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:50
#: mmcls.models.backbones.t2t_vit.T2T_ViT:38
#: mmcls.models.utils.attention.MultiheadAttention:35 of
msgid "The Config for initialization. Defaults to None."
msgstr ""

#: mmcls.models.backbones.swin_transformer.SwinTransformer:55 of
msgid "实际案例"
msgstr ""

#: mmcls.models.backbones.t2t_vit.T2T_ViT:1 of
msgid "Tokens-to-Token Vision Transformer (T2T-ViT)"
msgstr ""

#: mmcls.models.backbones.t2t_vit.T2T_ViT:3 of
msgid ""
"A PyTorch implementation of `Tokens-to-Token ViT: Training Vision "
"Transformers from Scratch on ImageNet "
"<https://arxiv.org/abs/2101.11986>`_"
msgstr ""

#: mmcls.models.backbones.t2t_vit.T2T_ViT:8
#: mmcls.models.utils.attention.ShiftWindowMSA:3 of
msgid "Number of input channels."
msgstr ""

#: mmcls.models.backbones.t2t_vit.T2T_ViT:10 of
msgid "Embedding dimension."
msgstr ""

#: mmcls.models.backbones.t2t_vit.T2T_ViT:12 of
msgid "Extra config of Tokens-to-Token module. Defaults to an empty dict."
msgstr ""

#: mmcls.models.backbones.t2t_vit.T2T_ViT:15 of
msgid "Dropout rate after position embedding. Defaults to 0."
msgstr ""

#: mmcls.models.backbones.t2t_vit.T2T_ViT:18 of
msgid "Num of transformer layers in encoder. Defaults to 14."
msgstr ""

#: mmcls.models.backbones.t2t_vit.T2T_ViT:24
#: mmcls.models.backbones.vision_transformer.VisionTransformer:36 of
msgid "Configs of each transformer layer in encoder. Defaults to an empty dict."
msgstr ""

#: mmcls.models.backbones.t2t_vit.T2T_ViT:32
#: mmcls.models.backbones.vision_transformer.VisionTransformer:25 of
msgid ""
"Whether to add a additional layer to normalize final feature map. "
"Defaults to True."
msgstr ""

#: mmcls.models.backbones.t2t_vit.T2T_ViT:35 of
msgid "Whether output the cls_token. Defaults to True."
msgstr ""

#: mmcls.models.backbones.timm_backbone.TIMMBackbone:1 of
msgid ""
"Wrapper to use backbones from timm library. More details can be found in "
"`timm <https://github.com/rwightman/pytorch-image-models>`_ ."
msgstr ""

#: mmcls.models.backbones.timm_backbone.TIMMBackbone:4 of
msgid "Name of timm model to instantiate."
msgstr ""

#: mmcls.models.backbones.timm_backbone.TIMMBackbone:6 of
msgid "Load pretrained weights if True."
msgstr ""

#: mmcls.models.backbones.timm_backbone.TIMMBackbone:8 of
msgid "Path of checkpoint to load after model is initialized."
msgstr ""

#: mmcls.models.backbones.timm_backbone.TIMMBackbone:13
#: mmcls.models.backbones.tnt.TNT:40 of
msgid "Initialization config dict"
msgstr ""

#: mmcls.models.backbones.timm_backbone.TIMMBackbone:15 of
msgid "Other timm & model specific arguments."
msgstr ""

#: mmcls.models.backbones.tnt.TNT:1 of
msgid ""
"Transformer in Transformer A PyTorch implement of : `Transformer in "
"Transformer <https://arxiv.org/abs/2103.00112>`_"
msgstr ""

#: mmcls.models.backbones.tnt.TNT:5 of
msgid ""
"Inspiration from https://github.com/rwightman/pytorch-image-"
"models/blob/master/timm/models/tnt.py"
msgstr ""

#: mmcls.models.backbones.tnt.TNT:8
#: mmcls.models.backbones.vision_transformer.VisionTransformer:7 of
msgid "Vision Transformer architecture Default: 'b'"
msgstr ""

#: mmcls.models.backbones.tnt.TNT:11 of
msgid "Input image size. Default to 224"
msgstr ""

#: mmcls.models.backbones.tnt.TNT:13 of
msgid "The patch size. Deault to 16"
msgstr ""

#: mmcls.models.backbones.tnt.TNT:15 of
msgid "Number of input channels. Default to 3"
msgstr ""

#: mmcls.models.backbones.tnt.TNT:17 of
msgid "A ratio to calculate the hidden_dims in ffn layer. Default: 4"
msgstr ""

#: mmcls.models.backbones.tnt.TNT:20 of
msgid "Enable bias for qkv if True. Default False"
msgstr ""

#: mmcls.models.backbones.tnt.TNT:22 of
msgid ""
"Probability of an element to be zeroed after the feed forward layer. "
"Default 0."
msgstr ""

#: mmcls.models.backbones.tnt.TNT:25 of
msgid "The drop out rate for attention layer. Default 0."
msgstr ""

#: mmcls.models.backbones.tnt.TNT:28 of
msgid "stochastic depth rate. Default 0."
msgstr ""

#: mmcls.models.backbones.tnt.TNT:30 of
msgid "The activation config for FFNs. Defaults to GELU."
msgstr ""

#: mmcls.models.backbones.tnt.TNT:32 of
msgid "Config dict for normalization layer. Default layer normalization"
msgstr ""

#: mmcls.models.backbones.tnt.TNT:35 of
msgid ""
"The stride of the conv2d layer. We use a conv2d layer and a unfold layer "
"to implement image to pixel embedding."
msgstr ""

#: mmcls.models.backbones.tnt.TNT:38 of
msgid "The number of fully-connected layers for FFNs. Default 2"
msgstr ""

#: mmcls.models.backbones.vgg.VGG:1 of
msgid "VGG backbone."
msgstr ""

#: mmcls.models.backbones.vgg.VGG:3 of
msgid "Depth of vgg, from {11, 13, 16, 19}."
msgstr ""

#: mmcls.models.backbones.vgg.VGG:5 of
msgid "Use BatchNorm or not."
msgstr ""

#: mmcls.models.backbones.vgg.VGG:7 of
msgid "number of classes for classification."
msgstr ""

#: mmcls.models.backbones.vgg.VGG:9 of
msgid "VGG stages, normally 5."
msgstr ""

#: mmcls.models.backbones.vgg.VGG:13 of
msgid ""
"Output from which stages. When it is None, the default behavior depends "
"on whether num_classes is specified. If num_classes <= 0, the default "
"value is (4, ), output the last feature map before classifier. If "
"num_classes > 0, the default value is (5, ), output the classification "
"score. Default: None."
msgstr ""

#: mmcls.models.backbones.vgg.VGG:20 of
msgid ""
"Stages to be frozen (all param fixed). -1 means not freezing any "
"parameters."
msgstr ""

#: mmcls.models.backbones.vgg.VGG:27 of
msgid "Whether to use ceil_mode of MaxPool. Default: False."
msgstr ""

#: mmcls.models.backbones.vgg.VGG:29 of
msgid "Whether to keep the last pooling before classifier. Default: True."
msgstr ""

#: mmcls.models.backbones.vision_transformer.VisionTransformer:1 of
msgid "Vision Transformer."
msgstr ""

#: mmcls.models.backbones.vision_transformer.VisionTransformer:3 of
msgid ""
"A PyTorch implement of : `An Image is Worth 16x16 Words: Transformers for"
" Image Recognition at Scale <https://arxiv.org/abs/2010.11929>`_"
msgstr ""

#: mmcls.models.backbones.vision_transformer.VisionTransformer:10 of
msgid "Input image size"
msgstr ""

#: mmcls.models.backbones.vision_transformer.VisionTransformer:12 of
msgid "The patch size"
msgstr ""

#: mmcls.models.backbones.vision_transformer.VisionTransformer:28 of
msgid ""
"Whether output the cls_token. If set True, `with_cls_token` must be True."
" Defaults to True."
msgstr ""

#: mmcls.models.backbones.vision_transformer.VisionTransformer:31 of
msgid ""
"Select the interpolate mode for position embeding vector resize. Defaults"
" to \"bicubic\"."
msgstr ""

#: mmcls.models.backbones.vision_transformer.VisionTransformer.resize_pos_embed:1
#: of
msgid "Resize pos_embed weights."
msgstr ""

#: mmcls.models.backbones.vision_transformer.VisionTransformer.resize_pos_embed:3
#: of
msgid "Position embedding weights with shape [1, L, C]."
msgstr ""

#: mmcls.models.backbones.vision_transformer.VisionTransformer.resize_pos_embed:6
#: of
msgid "The resolution of downsampled origin training image."
msgstr ""

#: mmcls.models.backbones.vision_transformer.VisionTransformer.resize_pos_embed:9
#: of
msgid "The resolution of downsampled new training image."
msgstr ""

#: mmcls.models.backbones.vision_transformer.VisionTransformer.resize_pos_embed:12
#: of
msgid ""
"Algorithm used for upsampling: ``'nearest'`` | ``'linear'`` | "
"``'bilinear'`` | ``'bicubic'`` | ``'trilinear'``. Default: ``'bicubic'``"
msgstr ""

#: mmcls.models.backbones.vision_transformer.VisionTransformer.resize_pos_embed:17
#: of
msgid "The resized pos_embed of shape [1, L_new, C]"
msgstr ""

#: ../../api.rst:33
msgid "heads"
msgstr ""

#: mmcls.models.heads.cls_head.ClsHead:1 of
msgid "classification head."
msgstr ""

#: mmcls.models.heads.cls_head.ClsHead:3
#: mmcls.models.heads.multi_label_head.MultiLabelClsHead:3
#: mmcls.models.heads.multi_label_linear_head.MultiLabelLinearClsHead:7 of
msgid "Config of classification loss."
msgstr ""

#: mmcls.models.heads.cls_head.ClsHead:5 of
msgid "Top-k accuracy."
msgstr ""

#: mmcls.models.heads.cls_head.ClsHead:7 of
msgid ""
"Whether to calculate accuracy during training. If you use Mixup/CutMix or"
" something like that during training, it is not reasonable to calculate "
"accuracy. Defaults to False."
msgstr ""

#: mmcls.models.heads.conformer_head.ConformerHead:1
#: mmcls.models.heads.linear_head.LinearClsHead:1 of
msgid "Linear classifier head."
msgstr ""

#: mmcls.models.heads.conformer_head.ConformerHead:3
#: mmcls.models.heads.linear_head.LinearClsHead:3
#: mmcls.models.heads.stacked_head.StackedLinearClsHead:3
#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead:3 of
msgid "Number of categories excluding the background category."
msgstr ""

#: mmcls.models.heads.conformer_head.ConformerHead:6
#: mmcls.models.heads.linear_head.LinearClsHead:6
#: mmcls.models.heads.multi_label_linear_head.MultiLabelLinearClsHead:5
#: mmcls.models.heads.stacked_head.StackedLinearClsHead:6
#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead:6 of
msgid "Number of channels in the input feature map."
msgstr ""

#: mmcls.models.heads.conformer_head.ConformerHead:8
#: mmcls.models.heads.linear_head.LinearClsHead:8
#: mmcls.models.heads.multi_label_linear_head.MultiLabelLinearClsHead:9 of
msgid ""
"The extra init config of layers. Defaults to use dict(type='Normal', "
"layer='Linear', std=0.01)."
msgstr ""

#: mmcls.models.heads.multi_label_head.MultiLabelClsHead:1 of
msgid "Classification head for multilabel task."
msgstr ""

#: mmcls.models.heads.multi_label_linear_head.MultiLabelLinearClsHead:1 of
msgid "Linear classification head for multilabel task."
msgstr ""

#: mmcls.models.heads.multi_label_linear_head.MultiLabelLinearClsHead:3 of
msgid "Number of categories."
msgstr ""

#: mmcls.models.heads.stacked_head.StackedLinearClsHead:1 of
msgid "Classifier head with several hidden fc layer and a output fc layer."
msgstr ""

#: mmcls.models.heads.stacked_head.StackedLinearClsHead:8 of
msgid "Number of channels in the hidden fc layers."
msgstr ""

#: mmcls.models.heads.stacked_head.StackedLinearClsHead:10 of
msgid ""
"Dropout rate after each hidden fc layer, except the last layer. Defaults "
"to 0."
msgstr ""

#: mmcls.models.heads.stacked_head.StackedLinearClsHead:13 of
msgid ""
"Config dict of normalization layer after each hidden fc layer, except the"
" last layer. Defaults to None."
msgstr ""

#: mmcls.models.heads.stacked_head.StackedLinearClsHead:16 of
msgid ""
"Config dict of activation function after each hidden layer, except the "
"last layer. Defaults to use \"ReLU\"."
msgstr ""

#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead:1 of
msgid "Vision Transformer classifier head."
msgstr ""

#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead:8 of
msgid ""
"Number of the dimensions for hidden layer. Only available during pre-"
"training. Default None."
msgstr ""

#: mmcls.models.heads.vision_transformer_head.VisionTransformerClsHead:11 of
msgid ""
"The activation config. Only available during pre-training. Defaults to "
"Tanh."
msgstr ""

#: ../../api.rst:38
msgid "necks"
msgstr ""

#: mmcls.models.necks.gap.GlobalAveragePooling:1 of
msgid "Global Average Pooling neck."
msgstr ""

#: mmcls.models.necks.gap.GlobalAveragePooling:3 of
msgid ""
"Note that we use `view` to remove extra channel after pooling. We do not "
"use `squeeze` as it will also remove the batch dimension when the tensor "
"has a batch dimension of size 1, which can lead to unexpected errors."
msgstr ""

#: mmcls.models.necks.gap.GlobalAveragePooling:7 of
msgid "Dimensions of each sample channel, can be one of {1, 2, 3}. Default: 2"
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss.forward:1
#: mmcls.models.necks.gap.GlobalAveragePooling.forward:1
#: mmcls.models.utils.attention.MultiheadAttention.forward:1
#: mmcls.models.utils.attention.ShiftWindowMSA.forward:1
#: mmcls.models.utils.embed.HybridEmbed.forward:1
#: mmcls.models.utils.embed.PatchEmbed.forward:1
#: mmcls.models.utils.inverted_residual.InvertedResidual.forward:1
#: mmcls.models.utils.se_layer.SELayer.forward:1 of
msgid "Defines the computation performed at every call."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss.forward:3
#: mmcls.models.necks.gap.GlobalAveragePooling.forward:3
#: mmcls.models.utils.attention.MultiheadAttention.forward:3
#: mmcls.models.utils.attention.ShiftWindowMSA.forward:3
#: mmcls.models.utils.embed.HybridEmbed.forward:3
#: mmcls.models.utils.embed.PatchEmbed.forward:3
#: mmcls.models.utils.inverted_residual.InvertedResidual.forward:3
#: mmcls.models.utils.se_layer.SELayer.forward:3 of
msgid "Should be overridden by all subclasses."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss.forward:6
#: mmcls.models.necks.gap.GlobalAveragePooling.forward:6
#: mmcls.models.utils.attention.MultiheadAttention.forward:6
#: mmcls.models.utils.attention.ShiftWindowMSA.forward:6
#: mmcls.models.utils.embed.HybridEmbed.forward:6
#: mmcls.models.utils.embed.PatchEmbed.forward:6
#: mmcls.models.utils.inverted_residual.InvertedResidual.forward:6
#: mmcls.models.utils.se_layer.SELayer.forward:6 of
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the registered hooks "
"while the latter silently ignores them."
msgstr ""

#: ../../api.rst:43
msgid "losses"
msgstr ""

#: mmcls.models.losses.accuracy.Accuracy.forward:1 of
msgid "Forward function to calculate accuracy."
msgstr ""

#: mmcls.models.losses.accuracy.Accuracy.forward:3 of
msgid "Prediction of models."
msgstr ""

#: mmcls.models.losses.accuracy.Accuracy.forward:5 of
msgid "Target for each prediction."
msgstr ""

#: mmcls.models.losses.accuracy.Accuracy.forward:8 of
msgid "The accuracies under different topk criterions."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss:1
#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss.forward:1
#: mmcls.models.losses.asymmetric_loss.asymmetric_loss:1 of
msgid "asymmetric loss."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss:3
#: mmcls.models.losses.asymmetric_loss.asymmetric_loss:14 of
msgid "positive focusing parameter. Defaults to 0.0."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss:6
#: mmcls.models.losses.asymmetric_loss.asymmetric_loss:16 of
msgid ""
"Negative focusing parameter. We usually set gamma_neg > gamma_pos. "
"Defaults to 4.0."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss:9
#: mmcls.models.losses.asymmetric_loss.asymmetric_loss:19 of
msgid "Probability margin. Defaults to 0.05."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss:11 of
msgid "The method used to reduce the loss into a scalar."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.AsymmetricLoss:14
#: mmcls.models.losses.focal_loss.FocalLoss:12 of
msgid "Weight of loss. Defaults to 1.0."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss:1 of
msgid "Cross entropy loss."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss:3 of
msgid "Whether the prediction uses sigmoid of softmax. Defaults to False."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss:6 of
msgid "Whether to use the soft version of CrossEntropyLoss. Defaults to False."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss:9
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:19 of
msgid ""
"The method used to reduce the loss. Options are \"none\", \"mean\" and "
"\"sum\". Defaults to 'mean'."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss:12
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:22 of
msgid "Weight of the loss. Defaults to 1.0."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss:14
#: mmcls.models.losses.cross_entropy_loss.binary_cross_entropy:17
#: mmcls.models.losses.cross_entropy_loss.cross_entropy:15 of
msgid ""
"The weight for each class with shape (C), C is the number of classes. "
"Default None."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.CrossEntropyLoss:17 of
msgid ""
"The positive weight for each class with shape (C), C is the number of "
"classes. Only enabled in BCE loss when ``use_sigmoid`` is True. Default "
"None."
msgstr ""

#: mmcls.models.losses.focal_loss.FocalLoss:1 of
msgid "Focal loss."
msgstr ""

#: mmcls.models.losses.focal_loss.FocalLoss:3 of
msgid "Focusing parameter in focal loss. Defaults to 2.0."
msgstr ""

#: mmcls.models.losses.focal_loss.FocalLoss:6 of
msgid "The parameter in balanced form of focal loss. Defaults to 0.25."
msgstr ""

#: mmcls.models.losses.focal_loss.FocalLoss:9 of
msgid ""
"The method used to reduce the loss into a scalar. Options are \"none\" "
"and \"mean\". Defaults to 'mean'."
msgstr ""

#: mmcls.models.losses.focal_loss.FocalLoss.forward:1
#: mmcls.models.losses.focal_loss.sigmoid_focal_loss:1 of
msgid "Sigmoid focal loss."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.asymmetric_loss:6
#: mmcls.models.losses.cross_entropy_loss.binary_cross_entropy:3
#: mmcls.models.losses.focal_loss.FocalLoss.forward:3
#: mmcls.models.losses.focal_loss.sigmoid_focal_loss:3
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward:3 of
msgid "The prediction with shape (N, \\*)."
msgstr ""

#: mmcls.models.losses.focal_loss.FocalLoss.forward:5 of
msgid "The ground truth label of the prediction with shape (N, \\*), N or (N,1)."
msgstr ""

#: mmcls.models.losses.focal_loss.FocalLoss.forward:8
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward:8 of
msgid "Sample-wise loss weight with shape (N, \\*). Defaults to None."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.asymmetric_loss:25
#: mmcls.models.losses.cross_entropy_loss.binary_cross_entropy:14
#: mmcls.models.losses.cross_entropy_loss.cross_entropy:12
#: mmcls.models.losses.focal_loss.FocalLoss.forward:11
#: mmcls.models.losses.focal_loss.sigmoid_focal_loss:20
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward:11
#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward:9 of
msgid "Average factor that is used to average the loss. Defaults to None."
msgstr ""

#: mmcls.models.losses.focal_loss.FocalLoss.forward:14
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward:14 of
msgid ""
"The method used to reduce the loss into a scalar. Options are \"none\", "
"\"mean\" and \"sum\". Defaults to None."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.asymmetric_loss:29
#: mmcls.models.losses.focal_loss.FocalLoss.forward:19
#: mmcls.models.losses.focal_loss.sigmoid_focal_loss:24
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward:19 of
msgid "Loss."
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:1 of
msgid "Initializer for the label smoothed cross entropy loss."
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:3 of
msgid ""
"Refers to `Rethinking the Inception Architecture for Computer Vision "
"<https://arxiv.org/abs/1512.00567>`_"
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:6 of
msgid ""
"This decreases gap between output scores and encourages generalization. "
"Labels provided to forward can be one-hot like vectors (NxC) or class "
"indices (Nx1). And this accepts linear combination of one-hot like labels"
" from mixup or cutmix except multi-label task."
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:12 of
msgid "The degree of label smoothing."
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:14 of
msgid "Number of classes. Defaults to None."
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:16 of
msgid ""
"Refers to notes, Options are 'original', 'classy_vision', 'multi_label'. "
"Defaults to 'classy_vision'"
msgstr ""

#: mmcls.datasets.pipelines.transforms.CenterCrop:23
#: mmcls.datasets.pipelines.transforms.RandomGrayscale:11
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:26 of
msgid "提示"
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:27 of
msgid ""
"if the mode is \"original\", this will use the same label smooth method "
"as the original paper as:"
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:30 of
msgid ""
"(1-\\epsilon)\\delta_{k, y} + \\frac{\\epsilon}{K}\n"
"\n"
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:33 of
msgid ""
"where epsilon is the `label_smooth_val`, K is the num_classes and "
"delta(k,y) is Dirac delta, which equals 1 for k=y and 0 otherwise."
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:36 of
msgid ""
"if the mode is \"classy_vision\", this will use the same label smooth "
"method as the facebookresearch/ClassyVision repo as:"
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:39 of
msgid ""
"\\frac{\\delta_{k, y} + \\epsilon/K}{1+\\epsilon}\n"
"\n"
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:42 of
msgid ""
"if the mode is \"multi_label\", this will accept labels from multi-label "
"task and smoothing them as:"
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss:45 of
msgid ""
"(1-2\\epsilon)\\delta_{k, y} + \\epsilon\n"
"\n"
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward:1 of
msgid "Label smooth loss."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.asymmetric_loss:8
#: mmcls.models.losses.focal_loss.sigmoid_focal_loss:5
#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.forward:5 of
msgid "The ground truth label of the prediction with shape (N, \\*)."
msgstr ""

#: mmcls.models.losses.label_smooth_loss.LabelSmoothLoss.generate_one_hot_like_label:1
#: of
msgid ""
"This function takes one-hot or index label vectors and computes one- hot "
"like label vectors (float)"
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss:1 of
msgid "Implementation of seesaw loss."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss:3 of
msgid ""
"Refers to `Seesaw Loss for Long-Tailed Instance Segmentation (CVPR 2021) "
"<https://arxiv.org/abs/2008.10032>`_"
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss:6 of
msgid ""
"Whether the prediction uses sigmoid of softmax. Only False is supported. "
"Defaults to False."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss:9 of
msgid "The ``p`` in the mitigation factor. Defaults to 0.8."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss:12 of
msgid "The ``q`` in the compenstation factor. Defaults to 2.0."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss:15 of
msgid "The number of classes. Default to 1000 for the ImageNet dataset."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss:18 of
msgid ""
"The minimal value of divisor to smooth the computation of compensation "
"factor, default to 1e-2."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss:21 of
msgid ""
"The method that reduces the loss to a scalar. Options are \"none\", "
"\"mean\" and \"sum\". Default to \"mean\"."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss:24 of
msgid "The weight of the loss. Defaults to 1.0"
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward:1 of
msgid "Forward function."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward:3 of
msgid "The prediction with shape (N, C)."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward:5 of
msgid "The learning label of the prediction."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.cross_entropy:8
#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward:7 of
msgid "Sample-wise loss weight."
msgstr ""

#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward:12 of
msgid ""
"The method used to reduce the loss. Options are \"none\", \"mean\" and "
"\"sum\"."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.binary_cross_entropy:24
#: mmcls.models.losses.cross_entropy_loss.cross_entropy:19
#: mmcls.models.losses.seesaw_loss.SeesawLoss.forward:16 of
msgid "The calculated loss"
msgstr ""

#: mmcls.models.losses.accuracy.accuracy:1 of
msgid "Calculate accuracy according to the prediction and target."
msgstr ""

#: mmcls.models.losses.accuracy.accuracy:3 of
msgid "The model prediction."
msgstr ""

#: mmcls.models.losses.accuracy.accuracy:5 of
msgid "The target of each prediction"
msgstr ""

#: mmcls.models.losses.accuracy.accuracy:7 of
msgid ""
"If the predictions in ``topk`` matches the target, the predictions will "
"be regarded as correct ones. Defaults to 1."
msgstr ""

#: mmcls.models.losses.accuracy.accuracy:15 of
msgid ""
"Accuracy     - torch.Tensor: If both ``topk`` and ``thrs`` is a single "
"value.     - list[torch.Tensor]: If one of ``topk`` or ``thrs`` is a "
"tuple.     - list[list[torch.Tensor]]: If both ``topk`` and ``thrs`` is a"
"               tuple. And the first dim is ``topk``, the second dim is "
"``thrs``."
msgstr ""

#: mmcls.models.losses.accuracy.accuracy:19 of
msgid "Accuracy"
msgstr ""

#: mmcls.models.losses.accuracy.accuracy:18 of
msgid "torch.Tensor: If both ``topk`` and ``thrs`` is a single value."
msgstr ""

#: mmcls.models.losses.accuracy.accuracy:19 of
msgid "list[torch.Tensor]: If one of ``topk`` or ``thrs`` is a tuple."
msgstr ""

#: mmcls.models.losses.accuracy.accuracy:20 of
msgid ""
"list[list[torch.Tensor]]: If both ``topk`` and ``thrs`` is a"
"               tuple. And the first dim is ``topk``, the second dim is "
"``thrs``."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.asymmetric_loss:3 of
msgid ""
"Please refer to the `paper <https://arxiv.org/abs/2009.14119>`__ for "
"details."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.asymmetric_loss:11
#: mmcls.models.losses.focal_loss.sigmoid_focal_loss:8 of
msgid "Sample-wise loss weight with shape (N, ). Defaults to None."
msgstr ""

#: mmcls.models.losses.asymmetric_loss.asymmetric_loss:21
#: mmcls.models.losses.cross_entropy_loss.binary_cross_entropy:10
#: mmcls.models.losses.focal_loss.sigmoid_focal_loss:16 of
msgid ""
"The method used to reduce the loss. Options are \"none\", \"mean\" and "
"\"sum\". If reduction is 'none' , loss is same shape as pred and label. "
"Defaults to 'mean'."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.binary_cross_entropy:1 of
msgid "Calculate the binary CrossEntropy loss with logits."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.binary_cross_entropy:5 of
msgid "The gt label with shape (N, \\*)."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.binary_cross_entropy:7 of
msgid "Element-wise weight of loss with shape (N, ). Defaults to None."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.binary_cross_entropy:20 of
msgid ""
"The positive weight for each class with shape (C), C is the number of "
"classes. Default None."
msgstr ""

#: mmcls.models.losses.utils.convert_to_one_hot:1 of
msgid ""
"This function converts target class indices to one-hot vectors, given the"
" number of classes."
msgstr ""

#: mmcls.models.losses.utils.convert_to_one_hot:4 of
msgid "The ground truth label of the prediction with shape (N, 1)"
msgstr ""

#: mmcls.models.losses.utils.convert_to_one_hot:7 of
msgid "the number of classes."
msgstr ""

#: mmcls.models.losses.utils.convert_to_one_hot:10
#: mmcls.models.losses.utils.weight_reduce_loss:12 of
msgid "Processed loss values."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.cross_entropy:1 of
msgid "Calculate the CrossEntropy loss."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.cross_entropy:3 of
msgid "The prediction with shape (N, C), C is the number of classes."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.cross_entropy:6 of
msgid "The gt label of the prediction."
msgstr ""

#: mmcls.models.losses.cross_entropy_loss.cross_entropy:10 of
msgid "The method used to reduce the loss."
msgstr ""

#: mmcls.models.losses.utils.reduce_loss:1 of
msgid "Reduce loss as specified."
msgstr ""

#: mmcls.models.losses.utils.reduce_loss:3 of
msgid "Elementwise loss tensor."
msgstr ""

#: mmcls.models.losses.utils.reduce_loss:5 of
msgid "Options are \"none\", \"mean\" and \"sum\"."
msgstr ""

#: mmcls.models.losses.utils.reduce_loss:8 of
msgid "Reduced loss tensor."
msgstr ""

#: mmcls.models.losses.focal_loss.sigmoid_focal_loss:11 of
msgid "The gamma for calculating the modulating factor. Defaults to 2.0."
msgstr ""

#: mmcls.models.losses.focal_loss.sigmoid_focal_loss:14 of
msgid "A balanced form for Focal Loss. Defaults to 0.25."
msgstr ""

#: mmcls.models.losses.utils.weight_reduce_loss:1 of
msgid "Apply element-wise weight and reduce loss."
msgstr ""

#: mmcls.models.losses.utils.weight_reduce_loss:3 of
msgid "Element-wise loss."
msgstr ""

#: mmcls.models.losses.utils.weight_reduce_loss:5 of
msgid "Element-wise weights."
msgstr ""

#: mmcls.models.losses.utils.weight_reduce_loss:7 of
msgid "Same as built-in losses of PyTorch."
msgstr ""

#: mmcls.models.losses.utils.weight_reduce_loss:9 of
msgid "Average factor when computing the mean of losses."
msgstr ""

#: mmcls.models.losses.utils.weighted_loss:1 of
msgid "Create a weighted version of a given loss function."
msgstr ""

#: mmcls.models.losses.utils.weighted_loss:3 of
msgid ""
"To use this decorator, the loss function must have the signature like "
"``loss_func(pred, target, **kwargs)``. The function only needs to compute"
" element-wise loss without any reduction. This decorator will add weight "
"and reduction arguments to the function. The decorated function will have"
" the signature like ``loss_func(pred, target, weight=None, "
"reduction='mean', avg_factor=None, **kwargs)``."
msgstr ""

#: mmcls.models.losses.utils.weighted_loss of
msgid "Example"
msgstr ""

#: ../../api.rst:48
msgid "utils"
msgstr ""

#: mmcls.models.utils.augment.augments.Augments:1 of
msgid "Data augments."
msgstr ""

#: mmcls.models.utils.augment.augments.Augments:3 of
msgid "We implement some data augmentation methods, such as mixup, cutmix."
msgstr ""

#: mmcls.models.utils.augment.augments.Augments:5 of
msgid "`mmcv.ConfigDict`): Config dict of augments"
msgstr ""

#: mmcls.models.utils.augment.augments.Augments:19 of
msgid ""
"To decide which augmentation within Augments block is used the following "
"rule is applied. We pick augmentation based on the probabilities. In the "
"example above, we decide if we should use BatchCutMix with probability "
"0.5, BatchMixup 0.3. As Identity is not in augments_cfg, we use Identity "
"with probability 1 - 0.5 - 0.3 = 0.2."
msgstr ""

#: mmcls.models.utils.embed.HybridEmbed:1 of
msgid "CNN Feature Map Embedding."
msgstr ""

#: mmcls.models.utils.embed.HybridEmbed:3 of
msgid "Extract feature map from CNN, flatten, project to embedding dim."
msgstr ""

#: mmcls.models.utils.embed.HybridEmbed:6 of
msgid "CNN backbone"
msgstr ""

#: mmcls.models.utils.embed.HybridEmbed:8 mmcls.models.utils.embed.PatchEmbed:5
#: of
msgid "The size of input image. Default: 224"
msgstr ""

#: mmcls.models.utils.embed.HybridEmbed:10 of
msgid "Size of feature map extracted by CNN backbone. Default: None"
msgstr ""

#: mmcls.models.utils.embed.HybridEmbed:13
#: mmcls.models.utils.embed.PatchEmbed:7 of
msgid "The num of input channels. Default: 3"
msgstr ""

#: mmcls.models.utils.embed.HybridEmbed:15
#: mmcls.models.utils.embed.PatchEmbed:9 of
msgid "The dimensions of embedding. Default: 768"
msgstr ""

#: mmcls.models.utils.embed.HybridEmbed:20 of
msgid "The Config for initialization. Default: None."
msgstr ""

#: mmcls.models.utils.inverted_residual.InvertedResidual:1 of
msgid "Inverted Residual Block."
msgstr ""

#: mmcls.models.utils.inverted_residual.InvertedResidual:3 of
msgid "The input channels of this Module."
msgstr ""

#: mmcls.models.utils.inverted_residual.InvertedResidual:5 of
msgid "The output channels of this Module."
msgstr ""

#: mmcls.models.utils.inverted_residual.InvertedResidual:7 of
msgid "The input channels of the depthwise convolution."
msgstr ""

#: mmcls.models.utils.inverted_residual.InvertedResidual:9 of
msgid "The kernel size of the depthwise convolution. Default: 3."
msgstr ""

#: mmcls.models.utils.inverted_residual.InvertedResidual:12 of
msgid "The stride of the depthwise convolution. Default: 1."
msgstr ""

#: mmcls.models.utils.inverted_residual.InvertedResidual:14 of
msgid "Config dict for se layer. Default: None, which means no se layer."
msgstr ""

#: mmcls.models.utils.inverted_residual.InvertedResidual:30 of
msgid "The output tensor."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:1 of
msgid "Multi-head Attention Module."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:3 of
msgid ""
"This module implements multi-head attention that supports different input"
" dims and embed dims. And it also supports a shortcut from ``value``, "
"which is useful if input dims is not the same with embed dims."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:7 of
msgid "The embedding dimension."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:9 of
msgid "Parallel attention heads."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:11 of
msgid "The input dimension, and if None, use ``embed_dims``. Defaults to None."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:14 of
msgid ""
"Dropout rate of the dropout layer after the attention calculation of "
"query and key. Defaults to 0."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:17 of
msgid ""
"Dropout rate of the dropout layer after the output projection. Defaults "
"to 0."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:20 of
msgid ""
"The dropout config before adding the shortcut. Defaults to "
"``dict(type='Dropout', drop_prob=0.)``."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:23 of
msgid "If True, add a learnable bias to q, k, v. Defaults to True."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:26 of
msgid ""
"Override default qk scale of ``head_dim ** -0.5`` if set. Defaults to "
"None."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:29 of
msgid "Defaults to True."
msgstr ""

#: mmcls.models.utils.attention.MultiheadAttention:31 of
msgid ""
"Add a shortcut from value to output. It's usually used if ``input_dims`` "
"is different from ``embed_dims``. Defaults to False."
msgstr ""

#: mmcls.models.utils.embed.PatchEmbed:1 of
msgid "Image to Patch Embedding."
msgstr ""

#: mmcls.models.utils.embed.PatchEmbed:3 of
msgid "We use a conv layer to implement PatchEmbed."
msgstr ""

#: mmcls.models.utils.embed.PatchEmbed:11 of
msgid "Config dict for normalization layer. Default: None"
msgstr ""

#: mmcls.models.utils.embed.PatchEmbed:14 of
msgid "The config dict for conv layers. Default: None"
msgstr ""

#: mmcls.models.utils.embed.PatchEmbed:17 of
msgid "The Config for initialization. Default: None"
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:1 of
msgid "Merge patch feature map."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:3 of
msgid ""
"This layer use nn.Unfold to group feature map by kernel_size, and use "
"norm and linear layer to embed grouped feature map."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:6 of
msgid "The size of input patch resolution."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:8 of
msgid "The num of input channels."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:10 of
msgid ""
"Expansion ratio of output channels. The num of output channels is equal "
"to int(expansion_ratio * in_channels)."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:13 of
msgid "the kernel size in the unfold layer. Defaults to 2."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:16 of
msgid ""
"the stride of the sliding blocks in the unfold layer. Defaults to be "
"equal with kernel_size."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:19 of
msgid "zero padding width in the unfold layer. Defaults to 0."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:22 of
msgid "dilation parameter in the unfold layer. Defaults to 1."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:25 of
msgid "Whether to add bias in linear layer or not. Defaults to False."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:28 of
msgid "Config dict for normalization layer. Defaults to dict(type='LN')."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging:31 of
msgid "The extra config for initialization. Defaults to None."
msgstr ""

#: mmcls.models.utils.embed.PatchMerging.forward:1 of
msgid "x: B, H*W, C"
msgstr ""

#: mmcls.models.utils.se_layer.SELayer:1 of
msgid "Squeeze-and-Excitation Module."
msgstr ""

#: mmcls.models.utils.se_layer.SELayer:3 of
msgid "The input (and output) channels of the SE layer."
msgstr ""

#: mmcls.models.utils.se_layer.SELayer:5 of
msgid ""
"The intermediate channel number of SElayer. Default: None, means the "
"value of ``squeeze_channels`` is ``make_divisible(channels // ratio, "
"divisor)``."
msgstr ""

#: mmcls.models.utils.se_layer.SELayer:9 of
msgid ""
"Squeeze ratio in SELayer, the intermediate channel will be "
"``make_divisible(channels // ratio, divisor)``. Only used when "
"``squeeze_channels`` is None. Default: 16."
msgstr ""

#: mmcls.models.utils.se_layer.SELayer:13 of
msgid ""
"The divisor to true divide the channel number. Only used when "
"``squeeze_channels`` is None. Default: 8."
msgstr ""

#: mmcls.models.utils.se_layer.SELayer:19 of
msgid ""
"Config dict for activation layer. If act_cfg is a dict, two activation "
"layers will be configurated by this dict. If act_cfg is a sequence of "
"dicts, the first activation layer will be configurated by the first dict "
"and the second activation layer will be configurated by the second dict. "
"Default: (dict(type='ReLU'), dict(type='Sigmoid'))"
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:1 of
msgid "Shift Window Multihead Self-Attention Module."
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:5 of
msgid "The resolution of the input feature map."
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:8 of
msgid "Number of attention heads."
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:10 of
msgid "The height and width of the window."
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:12 of
msgid ""
"The shift step of each window towards right-bottom. If zero, act as "
"regular window-msa. Defaults to 0."
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:15 of
msgid "If True, add a learnable bias to q, k, v. Default: True"
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:18 of
msgid "Override default qk scale of head_dim ** -0.5 if set. Defaults to None."
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:21 of
msgid "Dropout ratio of attention weight. Defaults to 0.0."
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:24 of
msgid "Dropout ratio of output. Defaults to 0."
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:26 of
msgid ""
"The dropout_layer used before output. Defaults to dict(type='DropPath', "
"drop_prob=0.)."
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:29 of
msgid ""
"Auto pad the feature map to be divisible by window_size, Defaults to "
"False."
msgstr ""

#: mmcls.models.utils.attention.ShiftWindowMSA:32 of
msgid "The extra config for initialization. Default: None."
msgstr ""

#: mmcls.models.utils.channel_shuffle.channel_shuffle:1 of
msgid "Channel Shuffle operation."
msgstr ""

#: mmcls.models.utils.channel_shuffle.channel_shuffle:3 of
msgid ""
"This function enables cross-group information flow for multiple groups "
"convolution layers."
msgstr ""

#: mmcls.models.utils.channel_shuffle.channel_shuffle:6 of
msgid "The input tensor."
msgstr ""

#: mmcls.models.utils.channel_shuffle.channel_shuffle:8 of
msgid "The number of groups to divide the input tensor in the channel dimension."
msgstr ""

#: mmcls.models.utils.channel_shuffle.channel_shuffle:12 of
msgid "The output tensor after channel shuffle operation."
msgstr ""

#: mmcls.models.utils.make_divisible.make_divisible:1 of
msgid "Make divisible function."
msgstr ""

#: mmcls.models.utils.make_divisible.make_divisible:3 of
msgid ""
"This function rounds the channel number down to the nearest value that "
"can be divisible by the divisor."
msgstr ""

#: mmcls.models.utils.make_divisible.make_divisible:6 of
msgid "The original channel number."
msgstr ""

#: mmcls.models.utils.make_divisible.make_divisible:8 of
msgid "The divisor to fully divide the channel number."
msgstr ""

#: mmcls.models.utils.make_divisible.make_divisible:10 of
msgid ""
"The minimum value of the output channel. Default: None, means that the "
"minimum value equal to the divisor."
msgstr ""

#: mmcls.models.utils.make_divisible.make_divisible:13 of
msgid ""
"The minimum ratio of the rounded channel number to the original channel "
"number. Default: 0.9."
msgstr ""

#: mmcls.models.utils.make_divisible.make_divisible:17 of
msgid "The modified output channel number"
msgstr ""

#: ../../api.rst:53
msgid "mmcls.datasets"
msgstr ""

#: ../../api.rst:56
msgid "datasets"
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:1 of
msgid "Base dataset."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:3
#: mmcls.datasets.imagenet21k.ImageNet21k:14 of
msgid "the prefix of data path"
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:5 of
msgid ""
"a list of dict, where each element represents a operation defined in "
"`mmcls.datasets.pipelines`"
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:8
#: mmcls.datasets.imagenet21k.ImageNet21k:19 of
msgid ""
"the annotation file. When ann_file is str, the subclass is expected to "
"read from the ann_file. When ann_file is None, the subclass is expected "
"to read according to data_prefix"
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset:12
#: mmcls.datasets.imagenet21k.ImageNet21k:23 of
msgid "in train mode or test mode"
msgstr ""

#: mmcls.datasets.BaseDataset.class_to_idx:1 of
msgid "Map mapping class name to class index."
msgstr ""

#: mmcls.datasets.BaseDataset.class_to_idx:3 of
msgid "mapping from class name to class index."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset.evaluate:1
#: mmcls.datasets.multi_label.MultiLabelDataset.evaluate:1 of
msgid "Evaluate the dataset."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset.evaluate:3
#: mmcls.datasets.multi_label.MultiLabelDataset.evaluate:3 of
msgid "Testing results of the dataset."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset.evaluate:5 of
msgid "Metrics to be evaluated. Default value is `accuracy`."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset.evaluate:8 of
msgid ""
"Options for calculating metrics. Allowed keys are 'topk', 'thrs' and "
"'average_mode'. Defaults to None."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset.evaluate:12
#: mmcls.datasets.multi_label.MultiLabelDataset.evaluate:12 of
msgid ""
"Logger used for printing related information during evaluation. Defaults "
"to None."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset.evaluate:16
#: mmcls.datasets.multi_label.MultiLabelDataset.evaluate:18 of
msgid "evaluation results"
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset.get_cat_ids:1
#: mmcls.datasets.imagenet21k.ImageNet21k.get_cat_ids:1 of
msgid "Get category id by index."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset.get_cat_ids:3
#: mmcls.datasets.imagenet21k.ImageNet21k.get_cat_ids:3
#: mmcls.datasets.multi_label.MultiLabelDataset.get_cat_ids:3 of
msgid "Index of data."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset.get_cat_ids:6
#: mmcls.datasets.imagenet21k.ImageNet21k.get_cat_ids:6 of
msgid "Image category of specified index."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset.get_classes:1 of
msgid "Get class names of current dataset."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset.get_classes:3 of
msgid ""
"If classes is None, use default CLASSES defined by builtin dataset. If "
"classes is a string, take it as a file name. The file contains the name "
"of classes where each line contains one class name. If classes is a tuple"
" or list, override the CLASSES defined by the dataset."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset.get_classes:10 of
msgid "Names of categories of the dataset."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset.get_gt_labels:1 of
msgid "Get all ground-truth labels (categories)."
msgstr ""

#: mmcls.datasets.base_dataset.BaseDataset.get_gt_labels:3 of
msgid "categories for all images."
msgstr ""

#: mmcls.datasets.cifar.CIFAR10:1 of
msgid "`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset."
msgstr ""

#: mmcls.datasets.cifar.CIFAR10:3 of
msgid ""
"This implementation is modified from "
"https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py"
msgstr ""

#: mmcls.datasets.cifar.CIFAR100:1 of
msgid "`CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset."
msgstr ""

#: mmcls.datasets.dataset_wrappers.ClassBalancedDataset:1 of
msgid "A wrapper of repeated dataset with repeat factor."
msgstr ""

#: mmcls.datasets.dataset_wrappers.ClassBalancedDataset:3 of
msgid ""
"Suitable for training on class imbalanced datasets like LVIS. Following "
"the sampling strategy in [#1]_, in each epoch, an image may appear "
"multiple times based on its \"repeat factor\"."
msgstr ""

#: mmcls.datasets.dataset_wrappers.ClassBalancedDataset:7 of
msgid ""
"The repeat factor for an image is a function of the frequency the rarest "
"category labeled in that image. The \"frequency of category c\" in [0, 1]"
" is defined by the fraction of images in the training set (without "
"repeats) in which category c appears."
msgstr ""

#: mmcls.datasets.dataset_wrappers.ClassBalancedDataset:12 of
msgid ""
"The dataset needs to implement :func:`self.get_cat_ids` to support "
"ClassBalancedDataset."
msgstr ""

#: mmcls.datasets.dataset_wrappers.ClassBalancedDataset:15 of
msgid "The repeat factor is computed as followed."
msgstr ""

#: mmcls.datasets.dataset_wrappers.ClassBalancedDataset:17 of
msgid ""
"For each category c, compute the fraction :math:`f(c)` of images that "
"contain it."
msgstr ""

#: mmcls.datasets.dataset_wrappers.ClassBalancedDataset:19 of
msgid "For each category c, compute the category-level repeat factor"
msgstr ""

#: mmcls.datasets.dataset_wrappers.ClassBalancedDataset:21 of
msgid ""
"r(c) = \\max(1, \\sqrt{\\frac{t}{f(c)}})\n"
"\n"
msgstr ""

#: mmcls.datasets.dataset_wrappers.ClassBalancedDataset:24 of
msgid ""
"For each image I and its labels :math:`L(I)`, compute the image-level "
"repeat factor"
msgstr ""

#: mmcls.datasets.dataset_wrappers.ClassBalancedDataset:27 of
msgid ""
"r(I) = \\max_{c \\in L(I)} r(c)\n"
"\n"
msgstr ""

#: mmcls.datasets.dataset_wrappers.ClassBalancedDataset:31 of
msgid "引用"
msgstr ""

#: mmcls.datasets.dataset_wrappers.ClassBalancedDataset:32 of
msgid "https://arxiv.org/pdf/1908.03195.pdf"
msgstr ""

#: mmcls.datasets.dataset_wrappers.ClassBalancedDataset:34
#: mmcls.datasets.dataset_wrappers.RepeatDataset:8 of
msgid "The dataset to be repeated."
msgstr ""

#: mmcls.datasets.dataset_wrappers.ClassBalancedDataset:36 of
msgid ""
"frequency threshold below which data is repeated. For categories with "
"`f_c` >= `oversample_thr`, there is no oversampling. For categories with "
"`f_c` < `oversample_thr`, the degree of oversampling following the "
"square-root inverse frequency heuristic above."
msgstr ""

#: mmcls.datasets.dataset_wrappers.ConcatDataset:1 of
msgid "A wrapper of concatenated dataset."
msgstr ""

#: mmcls.datasets.dataset_wrappers.ConcatDataset:3 of
msgid ""
"Same as :obj:`torch.utils.data.dataset.ConcatDataset`, but add "
"`get_cat_ids` function."
msgstr ""

#: mmcls.datasets.dataset_wrappers.ConcatDataset:6 of
msgid "A list of datasets."
msgstr ""

#: mmcls.datasets.mnist.FashionMNIST:1 of
msgid ""
"`Fashion-MNIST <https://github.com/zalandoresearch/fashion-mnist>`_ "
"Dataset."
msgstr ""

#: mmcls.datasets.imagenet.ImageNet:1 of
msgid "`ImageNet <http://www.image-net.org>`_ Dataset."
msgstr ""

#: mmcls.datasets.imagenet.ImageNet:3 of
msgid ""
"This implementation is modified from "
"https://github.com/pytorch/vision/blob/master/torchvision/datasets/imagenet.py"
msgstr ""

#: mmcls.datasets.imagenet21k.ImageNet21k:1 of
msgid "ImageNet21k Dataset."
msgstr ""

#: mmcls.datasets.imagenet21k.ImageNet21k:3 of
msgid ""
"Since the dataset ImageNet21k is extremely big, cantains 21k+ classes and"
" 1.4B files. This class has improved the following points on the basis of"
" the class ``ImageNet``, in order to save memory usage and time required "
":"
msgstr ""

#: mmcls.datasets.imagenet21k.ImageNet21k:8 of
msgid "Delete the samples attribute"
msgstr ""

#: mmcls.datasets.imagenet21k.ImageNet21k:9 of
msgid "using 'slots' create a Data_item tp replace dict"
msgstr ""

#: mmcls.datasets.imagenet21k.ImageNet21k:10 of
msgid ""
"Modify setting ``info`` dict from function ``load_annotations`` to "
"function ``prepare_data``"
msgstr ""

#: mmcls.datasets.imagenet21k.ImageNet21k:12 of
msgid "using int instead of np.array(..., np.int64)"
msgstr ""

#: mmcls.datasets.imagenet21k.ImageNet21k:16 of
msgid ""
"a list of dict, where each element represents a operation defined in "
"``mmcls.datasets.pipelines``"
msgstr ""

#: mmcls.datasets.imagenet21k.ImageNet21k:25 of
msgid "use multi label or not."
msgstr ""

#: mmcls.datasets.imagenet21k.ImageNet21k:27 of
msgid ""
"whether to use sub-directory pictures, which are meet the conditions in "
"the folder under category directory."
msgstr ""

#: mmcls.datasets.imagenet21k.ImageNet21k.load_annotations:1 of
msgid "load dataset annotations."
msgstr ""

#: mmcls.datasets.mnist.MNIST:1 of
msgid "`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset."
msgstr ""

#: mmcls.datasets.mnist.MNIST:3 of
msgid ""
"This implementation is modified from "
"https://github.com/pytorch/vision/blob/master/torchvision/datasets/mnist.py"
msgstr ""

#: mmcls.datasets.multi_label.MultiLabelDataset:1 of
msgid "Multi-label Dataset."
msgstr ""

#: mmcls.datasets.multi_label.MultiLabelDataset.evaluate:5 of
msgid ""
"Metrics to be evaluated. Default value is 'mAP'. Options are 'mAP', 'CP',"
" 'CR', 'CF1', 'OP', 'OR' and 'OF1'."
msgstr ""

#: mmcls.datasets.multi_label.MultiLabelDataset.evaluate:9 of
msgid ""
"Options for calculating metrics. Allowed keys are 'k' and 'thr'. Defaults"
" to None"
msgstr ""

#: mmcls.datasets.multi_label.MultiLabelDataset.evaluate:15 of
msgid "Used for containing deprecated arguments."
msgstr ""

#: mmcls.datasets.multi_label.MultiLabelDataset.get_cat_ids:1 of
msgid "Get category ids by index."
msgstr ""

#: mmcls.datasets.multi_label.MultiLabelDataset.get_cat_ids:6 of
msgid "Image categories of specified index."
msgstr ""

#: mmcls.datasets.dataset_wrappers.RepeatDataset:1 of
msgid "A wrapper of repeated dataset."
msgstr ""

#: mmcls.datasets.dataset_wrappers.RepeatDataset:3 of
msgid ""
"The length of repeated dataset will be `times` larger than the original "
"dataset. This is useful when the data loading time is long but the "
"dataset is small. Using RepeatDataset can reduce the data loading time "
"between epochs."
msgstr ""

#: mmcls.datasets.dataset_wrappers.RepeatDataset:10 of
msgid "Repeat times."
msgstr ""

#: mmcls.datasets.voc.VOC:1 of
msgid "`Pascal VOC <http://host.robots.ox.ac.uk/pascal/VOC/>`_ Dataset."
msgstr ""

#: mmcls.datasets.voc.VOC.load_annotations:1 of
msgid "Load annotations."
msgstr ""

#: mmcls.datasets.voc.VOC.load_annotations:3 of
msgid "Annotation info from XML file."
msgstr ""

#: mmcls.datasets.builder.build_dataloader:1 of
msgid "Build PyTorch DataLoader."
msgstr ""

#: mmcls.datasets.builder.build_dataloader:3 of
msgid ""
"In distributed training, each GPU/process has a dataloader. In non-"
"distributed training, there is only one dataloader for all GPUs."
msgstr ""

#: mmcls.datasets.builder.build_dataloader:6 of
msgid "A PyTorch dataset."
msgstr ""

#: mmcls.datasets.builder.build_dataloader:8 of
msgid "Number of training samples on each GPU, i.e., batch size of each GPU."
msgstr ""

#: mmcls.datasets.builder.build_dataloader:11 of
msgid "How many subprocesses to use for data loading for each GPU."
msgstr ""

#: mmcls.datasets.builder.build_dataloader:14 of
msgid "Number of GPUs. Only used in non-distributed training."
msgstr ""

#: mmcls.datasets.builder.build_dataloader:16 of
msgid "Distributed training/test or not. Default: True."
msgstr ""

#: mmcls.datasets.builder.build_dataloader:18 of
msgid "Whether to shuffle the data at every epoch. Default: True."
msgstr ""

#: mmcls.datasets.builder.build_dataloader:21 of
msgid ""
"Whether to round up the length of dataset by adding extra samples to make"
" it evenly divisible. Default: True."
msgstr ""

#: mmcls.datasets.builder.build_dataloader:24 of
msgid "Whether to use pin_memory in DataLoader. Default: True"
msgstr ""

#: mmcls.datasets.builder.build_dataloader:27 of
msgid ""
"If True, the data loader will not shutdown the worker processes after a "
"dataset has been consumed once. This allows to maintain the workers "
"Dataset instances alive. The argument also has effect in PyTorch>=1.7.0. "
"Default: True"
msgstr ""

#: mmcls.datasets.builder.build_dataloader:33 of
msgid "any keyword argument to be used to initialize DataLoader"
msgstr ""

#: ../../api.rst:61
msgid "pipelines"
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.AutoAugment:1 of
msgid "Auto augmentation."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.AutoAugment:3 of
msgid ""
"This data augmentation is proposed in `AutoAugment: Learning Augmentation"
" Policies from Data <https://arxiv.org/abs/1805.09501>`_."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.AutoAugment:6 of
msgid ""
"The policies of auto augmentation. Each policy in ``policies`` is a "
"specific augmentation policy, and is composed by several augmentations "
"(dict). When AutoAugment is called, a random policy in ``policies`` will "
"be selected to augment images."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.AutoAugment:12
#: mmcls.datasets.pipelines.auto_augment.RandAugment:34 of
msgid ""
"Configs of hyperparameters. Hyperparameters will be used in policies that"
" require these arguments if these arguments are not set in policy dicts. "
"Defaults to use _HPARAMS_DEFAULT."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.AutoContrast:1 of
msgid "Auto adjust image contrast."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.AutoContrast:3
#: mmcls.datasets.pipelines.auto_augment.Equalize:3
#: mmcls.datasets.pipelines.auto_augment.Invert:3 of
msgid ""
"The probability for performing invert therefore should be in range [0, "
"1]. Defaults to 0.5."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Brightness:1 of
msgid "Adjust images brightness."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Brightness:3 of
msgid ""
"The magnitude used for adjusting brightness. A positive magnitude would "
"enhance the brightness and a negative magnitude would make the image "
"darker. A magnitude=0 gives the origin img."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Brightness:8
#: mmcls.datasets.pipelines.auto_augment.Contrast:8
#: mmcls.datasets.pipelines.auto_augment.Sharpness:8 of
msgid ""
"The probability for performing contrast adjusting therefore should be in "
"range [0, 1]. Defaults to 0.5."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Brightness:11
#: mmcls.datasets.pipelines.auto_augment.ColorTransform:10
#: mmcls.datasets.pipelines.auto_augment.Contrast:11
#: mmcls.datasets.pipelines.auto_augment.Sharpness:11
#: mmcls.datasets.pipelines.auto_augment.Shear:15
#: mmcls.datasets.pipelines.auto_augment.Translate:18 of
msgid ""
"The probability that turns the magnitude negative, which should be in "
"range [0,1]. Defaults to 0.5."
msgstr ""

#: mmcls.datasets.pipelines.transforms.CenterCrop:1 of
msgid "Center crop the image."
msgstr ""

#: mmcls.datasets.pipelines.transforms.CenterCrop:3 of
msgid "Expected size after cropping with the format of (h, w)."
msgstr ""

#: mmcls.datasets.pipelines.transforms.CenterCrop:6 of
msgid "Whether to use efficientnet style center crop. Defaults to False."
msgstr ""

#: mmcls.datasets.pipelines.transforms.CenterCrop:9 of
msgid ""
"The crop padding parameter in efficientnet style center crop. Only valid "
"if efficientnet style is True. Defaults to 32."
msgstr ""

#: mmcls.datasets.pipelines.transforms.CenterCrop:13 of
msgid ""
"Interpolation method, accepted values are 'nearest', 'bilinear', "
"'bicubic', 'area', 'lanczos'. Only valid if ``efficientnet_style`` is "
"True. Defaults to 'bilinear'."
msgstr ""

#: mmcls.datasets.pipelines.transforms.CenterCrop:17 of
msgid ""
"The image resize backend type, accepted values are `cv2` and `pillow`. "
"Only valid if efficientnet style is True. Defaults to `cv2`."
msgstr ""

#: mmcls.datasets.pipelines.transforms.CenterCrop:24 of
msgid "If the image is smaller than the crop size, return the original image."
msgstr ""

#: mmcls.datasets.pipelines.transforms.CenterCrop:26 of
msgid ""
"If efficientnet_style is set to False, the pipeline would be a simple "
"center crop using the crop_size."
msgstr ""

#: mmcls.datasets.pipelines.transforms.CenterCrop:28 of
msgid ""
"If efficientnet_style is set to True, the pipeline will be to first to "
"perform the center crop with the ``crop_size_`` as:"
msgstr ""

#: mmcls.datasets.pipelines.transforms.CenterCrop:31 of
msgid ""
"\\text{crop\\_size\\_} = \\frac{\\text{crop\\_size}}{\\text{crop\\_size} "
"+\n"
"\\text{crop\\_padding}} \\times \\text{short\\_edge}\n"
"\n"
msgstr ""

#: mmcls.datasets.pipelines.transforms.CenterCrop:35 of
msgid "And then the pipeline resizes the img to the input crop size."
msgstr ""

#: mmcls.datasets.pipelines.formatting.Collect:1 of
msgid "Collect data from the loader relevant to the specific task."
msgstr ""

#: mmcls.datasets.pipelines.formatting.Collect:3 of
msgid ""
"This is usually the last stage of the data loader pipeline. Typically "
"keys is set to some subset of \"img\" and \"gt_label\"."
msgstr ""

#: mmcls.datasets.pipelines.formatting.Collect:6 of
msgid "Keys of results to be collected in ``data``."
msgstr ""

#: mmcls.datasets.pipelines.formatting.Collect:8 of
msgid ""
"Meta keys to be converted to ``mmcv.DataContainer`` and collected in "
"``data[img_metas]``. Default: ('filename', 'ori_shape', 'img_shape', "
"'flip', 'flip_direction', 'img_norm_cfg')"
msgstr ""

#: mmcls.datasets.pipelines.formatting.Collect:14 of
msgid ""
"The result dict contains the following keys      - keys in ``self.keys``"
"     - ``img_metas`` if available"
msgstr ""

#: mmcls.datasets.pipelines.formatting.Collect:16 of
msgid "The result dict contains the following keys"
msgstr ""

#: mmcls.datasets.pipelines.formatting.Collect:18 of
msgid "keys in ``self.keys``"
msgstr ""

#: mmcls.datasets.pipelines.formatting.Collect:19 of
msgid "``img_metas`` if available"
msgstr ""

#: mmcls.datasets.pipelines.transforms.ColorJitter:1 of
msgid "Randomly change the brightness, contrast and saturation of an image."
msgstr ""

#: mmcls.datasets.pipelines.transforms.ColorJitter:3 of
msgid ""
"How much to jitter brightness. brightness_factor is chosen uniformly from"
" [max(0, 1 - brightness), 1 + brightness]."
msgstr ""

#: mmcls.datasets.pipelines.transforms.ColorJitter:7 of
msgid ""
"How much to jitter contrast. contrast_factor is chosen uniformly from "
"[max(0, 1 - contrast), 1 + contrast]."
msgstr ""

#: mmcls.datasets.pipelines.transforms.ColorJitter:11 of
msgid ""
"How much to jitter saturation. saturation_factor is chosen uniformly from"
" [max(0, 1 - saturation), 1 + saturation]."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.ColorTransform:1 of
msgid "Adjust images color balance."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.ColorTransform:3 of
msgid ""
"The magnitude used for color transform. A positive magnitude would "
"enhance the color and a negative magnitude would make the image grayer. A"
" magnitude=0 gives the origin img."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.ColorTransform:7 of
msgid ""
"The probability for performing ColorTransform therefore should be in "
"range [0, 1]. Defaults to 0.5."
msgstr ""

#: mmcls.datasets.pipelines.compose.Compose:1 of
msgid "Compose a data pipeline with a sequence of transforms."
msgstr ""

#: mmcls.datasets.pipelines.compose.Compose:3 of
msgid "Either config dicts of transforms or transform objects."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Contrast:1 of
msgid "Adjust images contrast."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Contrast:3 of
msgid ""
"The magnitude used for adjusting contrast. A positive magnitude would "
"enhance the contrast and a negative magnitude would make the image "
"grayer. A magnitude=0 gives the origin img."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Cutout:1 of
msgid "Cutout images."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Cutout:3 of
msgid ""
"Expected cutout shape (h, w). If given as a single value, the value will "
"be used for both h and w."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Cutout:7 of
msgid ""
"Pixel pad_val value for constant fill. If it is a sequence, it must have "
"the same length with the image channels. Defaults to 128."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Cutout:11 of
msgid ""
"The probability for performing cutout therefore should be in range [0, "
"1]. Defaults to 0.5."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Equalize:1 of
msgid "Equalize the image histogram."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Invert:1 of
msgid "Invert images."
msgstr ""

#: mmcls.datasets.pipelines.transforms.Lighting:1 of
msgid "Adjust images lighting using AlexNet-style PCA jitter."
msgstr ""

#: mmcls.datasets.pipelines.transforms.Lighting:3 of
msgid "the eigenvalue of the convariance matrix of pixel values, respectively."
msgstr ""

#: mmcls.datasets.pipelines.transforms.Lighting:6 of
msgid "the eigenvector of the convariance matrix of pixel values, respectively."
msgstr ""

#: mmcls.datasets.pipelines.transforms.Lighting:9 of
msgid "The standard deviation for distribution of alpha. Defaults to 0.1"
msgstr ""

#: mmcls.datasets.pipelines.transforms.Lighting:12 of
msgid "Whether to convert img to rgb."
msgstr ""

#: mmcls.datasets.pipelines.loading.LoadImageFromFile:1 of
msgid "Load an image from file."
msgstr ""

#: mmcls.datasets.pipelines.loading.LoadImageFromFile:3 of
msgid ""
"Required keys are \"img_prefix\" and \"img_info\" (a dict that must "
"contain the key \"filename\"). Added or updated keys are \"filename\", "
"\"img\", \"img_shape\", \"ori_shape\" (same as `img_shape`) and "
"\"img_norm_cfg\" (means=0 and stds=1)."
msgstr ""

#: mmcls.datasets.pipelines.loading.LoadImageFromFile:7 of
msgid ""
"Whether to convert the loaded image to a float32 numpy array. If set to "
"False, the loaded image is an uint8 array. Defaults to False."
msgstr ""

#: mmcls.datasets.pipelines.loading.LoadImageFromFile:11 of
msgid "The flag argument for :func:`mmcv.imfrombytes()`. Defaults to 'color'."
msgstr ""

#: mmcls.datasets.pipelines.loading.LoadImageFromFile:14 of
msgid ""
"Arguments to instantiate a FileClient. See "
":class:`mmcv.fileio.FileClient` for details. Defaults to "
"``dict(backend='disk')``."
msgstr ""

#: mmcls.datasets.pipelines.transforms.Normalize:1 of
msgid "Normalize the image."
msgstr ""

#: mmcls.datasets.pipelines.transforms.Normalize:3 of
msgid "Mean values of 3 channels."
msgstr ""

#: mmcls.datasets.pipelines.transforms.Normalize:5 of
msgid "Std values of 3 channels."
msgstr ""

#: mmcls.datasets.pipelines.transforms.Normalize:7 of
msgid "Whether to convert the image from BGR to RGB, default is true."
msgstr ""

#: mmcls.datasets.pipelines.transforms.Pad:1 of
msgid "Pad images."
msgstr ""

#: mmcls.datasets.pipelines.transforms.Pad:3 of
msgid ""
"Expected padding size (h, w). Conflicts with pad_to_square. Defaults to "
"None."
msgstr ""

#: mmcls.datasets.pipelines.transforms.Pad:6 of
msgid "Pad any image to square shape. Defaults to False."
msgstr ""

#: mmcls.datasets.pipelines.transforms.Pad:8 of
msgid ""
"Values to be filled in padding areas when padding_mode is 'constant'. "
"Default to 0."
msgstr ""

#: mmcls.datasets.pipelines.transforms.Pad:11 of
msgid ""
"Type of padding. Should be: constant, edge, reflect or symmetric. Default"
" to \"constant\"."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Posterize:1 of
msgid "Posterize images (reduce the number of bits for each color channel)."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Posterize:3 of
msgid ""
"Number of bits for each pixel in the output img, which should be less or "
"equal to 8."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Posterize:6 of
msgid ""
"The probability for posterizing therefore should be in range [0, 1]. "
"Defaults to 0.5."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.RandAugment:1 of
msgid "Random augmentation."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.RandAugment:3 of
msgid ""
"This data augmentation is proposed in `RandAugment: Practical automated "
"data augmentation with a reduced search space "
"<https://arxiv.org/abs/1909.13719>`_."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.RandAugment:7 of
msgid ""
"The policies of random augmentation. Each policy in ``policies`` is one "
"specific augmentation policy (dict). The policy shall at least have key "
"`type`, indicating the type of augmentation. For those which have "
"magnitude, (given to the fact they are named differently in different "
"augmentation, ) `magnitude_key` and `magnitude_range` shall be the "
"magnitude argument (str) and the range of magnitude (tuple in the format "
"of (val1, val2)), respectively. Note that val1 is not necessarily less "
"than val2."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.RandAugment:17 of
msgid "Number of policies to select from policies each time."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.RandAugment:20 of
msgid "Magnitude level for all the augmentation selected."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.RandAugment:23 of
msgid "Total level for the magnitude. Defaults to 30."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.RandAugment:26 of
msgid ""
"Deviation of magnitude noise applied.  - If positive number, magnitude is"
" sampled from normal distribution   (mean=magnitude, std=magnitude_std). "
"- If 0 or negative number, magnitude remains unchanged. - If str \"inf\","
" magnitude is sampled from uniform distribution   (range=[min, "
"magnitude])."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.RandAugment:26 of
msgid "Deviation of magnitude noise applied."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.RandAugment:28 of
msgid ""
"If positive number, magnitude is sampled from normal distribution "
"(mean=magnitude, std=magnitude_std)."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.RandAugment:30 of
msgid "If 0 or negative number, magnitude remains unchanged."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.RandAugment:31 of
msgid ""
"If str \"inf\", magnitude is sampled from uniform distribution "
"(range=[min, magnitude])."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.RandAugment:41 of
msgid ""
"`magnitude_std` will introduce some randomness to policy, modified by "
"https://github.com/rwightman/pytorch-image-models."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.RandAugment:44 of
msgid "When magnitude_std=0, we calculate the magnitude as follows:"
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.RandAugment:46 of
msgid ""
"\\text{magnitude} = \\frac{\\text{magnitude\\_level}}\n"
"{\\text{total\\_level}} \\times (\\text{val2} - \\text{val1})\n"
"+ \\text{val1}\n"
"\n"
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomCrop:1 of
msgid "Crop the given Image at a random location."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomCrop:3
#: mmcls.datasets.pipelines.transforms.RandomResizedCrop:7 of
msgid ""
"Desired output size of the crop. If size is an int instead of sequence "
"like (h, w), a square crop (size, size) is made."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomCrop:7 of
msgid ""
"Optional padding on each border of the image. If a sequence of length 4 "
"is provided, it is used to pad left, top, right, bottom borders "
"respectively.  If a sequence of length 2 is provided, it is used to pad "
"left/right, top/bottom borders, respectively. Default: None, which means "
"no padding."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomCrop:13 of
msgid ""
"It will pad the image if smaller than the desired size to avoid raising "
"an exception. Since cropping is done after padding, the padding seems to "
"be done at a random offset. Default: False."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomCrop:18 of
msgid ""
"Pixel pad_val value for constant fill. If a tuple of length 3, it is used"
" to pad_val R, G, B channels respectively. Default: 0."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomCrop:22 of
msgid ""
"Type of padding. Defaults to \"constant\". Should be one of the "
"following:  - constant: Pads with a constant value, this value is "
"specified                 with pad_val. - edge: pads with the last value "
"at the edge of the image. - reflect: Pads with reflection of image "
"without repeating the                 last value on the edge. For "
"example, padding [1, 2, 3, 4]                 with 2 elements on both "
"sides in reflect mode will result                 in [3, 2, 1, 2, 3, 4, "
"3, 2]. - symmetric: Pads with reflection of image repeating the last"
"                 value on the edge. For example, padding [1, 2, 3, 4] "
"with                 2 elements on both sides in symmetric mode will "
"result in                 [2, 1, 1, 2, 3, 4, 4, 3]."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomCrop:22 of
msgid "Type of padding. Defaults to \"constant\". Should be one of the following:"
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomCrop:25 of
msgid ""
"constant: Pads with a constant value, this value is specified"
"                 with pad_val."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomCrop:26 of
msgid "edge: pads with the last value at the edge of the image."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomCrop:27 of
msgid ""
"reflect: Pads with reflection of image without repeating the"
"                 last value on the edge. For example, padding [1, 2, 3, "
"4]                 with 2 elements on both sides in reflect mode will "
"result                 in [3, 2, 1, 2, 3, 4, 3, 2]."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomCrop:28 of
msgid ""
"symmetric: Pads with reflection of image repeating the last"
"                 value on the edge. For example, padding [1, 2, 3, 4] "
"with                 2 elements on both sides in symmetric mode will "
"result in                 [2, 1, 1, 2, 3, 4, 4, 3]."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomCrop.get_params:1 of
msgid "Get parameters for ``crop`` for a random crop."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomCrop.get_params:3
#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params:3
#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params_efficientnet_style:4
#: of
msgid "Image to be cropped."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomCrop.get_params:5 of
msgid "Expected output size of the crop."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomCrop.get_params:8 of
msgid ""
"Params (xmin, ymin, target_height, target_width) to be     passed to "
"``crop`` for random crop."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomCrop.get_params:10 of
msgid "Params (xmin, ymin, target_height, target_width) to be"
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomCrop.get_params:11 of
msgid "passed to ``crop`` for random crop."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomErasing:1 of
msgid "Randomly selects a rectangle region in an image and erase pixels."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomErasing:3 of
msgid "Probability that image will be randomly erased. Default: 0.5"
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomErasing:6 of
msgid "Minimum erased area / input image area Default: 0.02"
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomErasing:9 of
msgid "Maximum erased area / input image area Default: 0.4"
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomErasing:12 of
msgid ""
"Aspect ratio range of erased area. if float, it will be converted to "
"(aspect_ratio, 1/aspect_ratio) Default: (3/10, 10/3)"
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomErasing:16 of
msgid ""
"Fill method in erased area, can be:  - const (default): All pixels are "
"assign with the same value. - rand: each pixel is assigned with a random "
"value in [0, 255]"
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomErasing:16 of
msgid "Fill method in erased area, can be:"
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomErasing:18 of
msgid "const (default): All pixels are assign with the same value."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomErasing:19 of
msgid "rand: each pixel is assigned with a random value in [0, 255]"
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomErasing:21 of
msgid "Base color filled in erased area. Defaults to (128, 128, 128)."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomErasing:24 of
msgid ""
"If set and ``mode`` is 'rand', fill erased area with random color from "
"normal distribution (mean=fill_color, std=fill_std); If not set, fill "
"erased area with random color from uniform distribution (0~255). Defaults"
" to None."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomErasing:32 of
msgid ""
"See `Random Erasing Data Augmentation "
"<https://arxiv.org/pdf/1708.04896.pdf>`_"
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomErasing:35 of
msgid ""
"This paper provided 4 modes: RE-R, RE-M, RE-0, RE-255, and use RE-M as "
"default. The config of these 4 modes are:"
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomErasing:38 of
msgid "RE-R: RandomErasing(mode='rand')"
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomErasing:39 of
msgid "RE-M: RandomErasing(mode='const', fill_color=(123.67, 116.3, 103.5))"
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomErasing:40 of
msgid "RE-0: RandomErasing(mode='const', fill_color=0)"
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomErasing:41 of
msgid "RE-255: RandomErasing(mode='const', fill_color=255)"
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomFlip:1 of
msgid "Flip the image randomly."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomFlip:3 of
msgid "Flip the image randomly based on flip probaility and flip direction."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomFlip:5 of
msgid "probability of the image being flipped. Default: 0.5"
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomFlip:7 of
msgid ""
"The flipping direction. Options are 'horizontal' and 'vertical'. Default:"
" 'horizontal'."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomGrayscale:1 of
msgid "Randomly convert image to grayscale with a probability of gray_prob."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomGrayscale:3 of
msgid "Probability that image should be converted to grayscale. Default: 0.1."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomGrayscale:7 of
msgid "Image after randomly grayscale transform."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomGrayscale:12 of
msgid "If input image is 1 channel: grayscale version is 1 channel."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomGrayscale:13 of
msgid ""
"If input image is 3 channel: grayscale version is 3 channel with r == g "
"== b."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop:1 of
msgid "Crop the given image to random size and aspect ratio."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop:3 of
msgid ""
"A crop of random size (default: of 0.08 to 1.0) of the original size and "
"a random aspect ratio (default: of 3/4 to 4/3) of the original aspect "
"ratio is made. This crop is finally resized to given size."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop:11 of
msgid ""
"Range of the random size of the cropped image compared to the original "
"image. Defaults to (0.08, 1.0)."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop:14 of
msgid ""
"Range of the random aspect ratio of the cropped image compared to the "
"original image. Defaults to (3. / 4., 4. / 3.)."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop:17 of
msgid ""
"Maximum number of attempts before falling back to Central Crop. Defaults "
"to 10."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop:20 of
msgid "Whether to use efficientnet style Random ResizedCrop. Defaults to False."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop:23
#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params_efficientnet_style:17
#: of
msgid ""
"Minimum ratio of the cropped area to the original area. Only valid if "
"efficientnet_style is true. Defaults to 0.1."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop:26 of
msgid ""
"The crop padding parameter in efficientnet style center crop. Only valid "
"if efficientnet_style is true. Defaults to 32."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop:30 of
msgid ""
"Interpolation method, accepted values are 'nearest', 'bilinear', "
"'bicubic', 'area', 'lanczos'. Defaults to 'bilinear'."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop:34 of
msgid ""
"The image resize backend type, accepted values are `cv2` and `pillow`. "
"Defaults to `cv2`."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params:1 of
msgid "Get parameters for ``crop`` for a random sized crop."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params:5
#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params_efficientnet_style:8
#: of
msgid ""
"Range of the random size of the cropped image compared to the original "
"image size."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params:8
#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params_efficientnet_style:11
#: of
msgid ""
"Range of the random aspect ratio of the cropped image compared to the "
"original image area."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params:11
#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params_efficientnet_style:14
#: of
msgid ""
"Maximum number of attempts before falling back to central crop. Defaults "
"to 10."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params:15
#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params_efficientnet_style:25
#: of
msgid ""
"Params (ymin, xmin, ymax, xmax) to be passed to `crop` for     a random "
"sized crop."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params:17
#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params_efficientnet_style:27
#: of
msgid "Params (ymin, xmin, ymax, xmax) to be passed to `crop` for"
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params:18
#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params_efficientnet_style:28
#: of
msgid "a random sized crop."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params_efficientnet_style:1
#: of
msgid "Get parameters for ``crop`` for a random sized crop in efficientnet style."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params_efficientnet_style:6
#: of
msgid "Desired output size of the crop."
msgstr ""

#: mmcls.datasets.pipelines.transforms.RandomResizedCrop.get_params_efficientnet_style:21
#: of
msgid ""
"The crop padding parameter in efficientnet style center crop. Defaults to"
" 32."
msgstr ""

#: mmcls.datasets.pipelines.transforms.Resize:1 of
msgid "Resize images."
msgstr ""

#: mmcls.datasets.pipelines.transforms.Resize:3 of
msgid ""
"Images scales for resizing (h, w). When size is int, the default behavior"
" is to resize an image to (size, size). When size is tuple and the second"
" value is -1, the image will be resized according to adaptive_side. For "
"example, when size is 224, the image is resized to 224x224. When size is "
"(224, -1) and adaptive_size is \"short\", the short side is resized to "
"224 and the other side is computed based on the short side, maintaining "
"the aspect ratio."
msgstr ""

#: mmcls.datasets.pipelines.transforms.Resize:12 of
msgid ""
"Interpolation method. For \"cv2\" backend, accepted values are "
"\"nearest\", \"bilinear\", \"bicubic\", \"area\", \"lanczos\". For "
"\"pillow\" backend, accepted values are \"nearest\", \"bilinear\", "
"\"bicubic\", \"box\", \"lanczos\", \"hamming\". More details can be found"
" in `mmcv.image.geometric`."
msgstr ""

#: mmcls.datasets.pipelines.transforms.Resize:18 of
msgid ""
"Adaptive resize policy, accepted values are \"short\", \"long\", "
"\"height\", \"width\". Default to \"short\"."
msgstr ""

#: mmcls.datasets.pipelines.transforms.Resize:21 of
msgid ""
"The image resize backend type, accepted values are `cv2` and `pillow`. "
"Default: `cv2`."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Rotate:1 of
msgid "Rotate images."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Rotate:3 of
msgid "The angle used for rotate. Positive values stand for clockwise rotation."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Rotate:6 of
msgid ""
"Center point (w, h) of the rotation in the source image. If None, the "
"center of the image will be used. Defaults to None."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Rotate:10 of
msgid "Isotropic scale factor. Defaults to 1.0."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Rotate:12
#: mmcls.datasets.pipelines.auto_augment.Shear:5
#: mmcls.datasets.pipelines.auto_augment.Translate:8 of
msgid ""
"Pixel pad_val value for constant fill. If a sequence of length 3, it is "
"used to pad_val R, G, B channels respectively. Defaults to 128."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Rotate:16 of
msgid ""
"The probability for performing Rotate therefore should be in range [0, "
"1]. Defaults to 0.5."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Rotate:19 of
msgid ""
"The probability that turns the angle negative, which should be in range "
"[0,1]. Defaults to 0.5."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Rotate:22
#: mmcls.datasets.pipelines.auto_augment.Translate:21 of
msgid ""
"Interpolation method. Options are 'nearest', 'bilinear', 'bicubic', "
"'area', 'lanczos'. Defaults to 'nearest'."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Sharpness:1 of
msgid "Adjust images sharpness."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Sharpness:3 of
msgid ""
"The magnitude used for adjusting sharpness. A positive magnitude would "
"enhance the sharpness and a negative magnitude would make the image bulr."
" A magnitude=0 gives the origin img."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Shear:1 of
msgid "Shear images."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Shear:3 of
msgid "The magnitude used for shear."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Shear:9 of
msgid ""
"The probability for performing Shear therefore should be in range [0, 1]."
" Defaults to 0.5."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Shear:12 of
msgid ""
"The shearing direction. Options are 'horizontal' and 'vertical'. Defaults"
" to 'horizontal'."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Shear:18 of
msgid ""
"Interpolation method. Options are 'nearest', 'bilinear', 'bicubic', "
"'area', 'lanczos'. Defaults to 'bicubic'."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Solarize:1 of
msgid "Solarize images (invert all pixel values above a threshold)."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Solarize:3 of
msgid "The threshold above which the pixels value will be inverted."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Solarize:6
#: mmcls.datasets.pipelines.auto_augment.SolarizeAdd:8 of
msgid ""
"The probability for solarizing therefore should be in range [0, 1]. "
"Defaults to 0.5."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.SolarizeAdd:1 of
msgid "SolarizeAdd images (add a certain value to pixels below a threshold)."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.SolarizeAdd:3 of
msgid "The value to be added to pixels below the thr."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.SolarizeAdd:5 of
msgid "The threshold below which the pixels value will be adjusted."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Translate:1 of
msgid "Translate images."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Translate:3 of
msgid ""
"The magnitude used for translate. Note that the offset is calculated by "
"magnitude * size in the corresponding direction. With a magnitude of 1, "
"the whole image will be moved out of the range."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Translate:12 of
msgid ""
"The probability for performing translate therefore should be in range [0,"
" 1]. Defaults to 0.5."
msgstr ""

#: mmcls.datasets.pipelines.auto_augment.Translate:15 of
msgid ""
"The translating direction. Options are 'horizontal' and 'vertical'. "
"Defaults to 'horizontal'."
msgstr ""

#: mmcls.datasets.pipelines.formatting.to_tensor:1 of
msgid "Convert objects of various python types to :obj:`torch.Tensor`."
msgstr ""

#: mmcls.datasets.pipelines.formatting.to_tensor:3 of
msgid ""
"Supported types are: :class:`numpy.ndarray`, :class:`torch.Tensor`, "
":class:`Sequence`, :class:`int` and :class:`float`."
msgstr ""

#: ../../api.rst:66
msgid "mmcls.utils"
msgstr ""

#: mmcls.utils.collect_env.collect_env:1 of
msgid "Collect the information of the running environments."
msgstr ""

#: mmcls.utils.logger.load_json_logs:1 of
msgid "load and convert json_logs to log_dicts."
msgstr ""

#: mmcls.utils.logger.load_json_logs:3 of
msgid "paths of json_logs."
msgstr ""

#: mmcls.utils.logger.load_json_logs:6 of
msgid ""
"dict())]: key is epoch, value is a sub dict keys of     sub dict is "
"different metrics, e.g. memory, bbox_mAP, value of     sub dict is a list"
" of corresponding values of all iterations."
msgstr ""

#: mmcls.utils.logger.load_json_logs:9 of
msgid "dict())]: key is epoch, value is a sub dict keys of"
msgstr ""

#: mmcls.utils.logger.load_json_logs:9 of
msgid ""
"sub dict is different metrics, e.g. memory, bbox_mAP, value of sub dict "
"is a list of corresponding values of all iterations."
msgstr ""
